{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Transformerの要素"],"metadata":{"id":"D1ewu_ttnVop"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import matplotlib.pyplot as plt"],"metadata":{"id":"zY6B3wLQDPho","executionInfo":{"status":"ok","timestamp":1745972308541,"user_tz":-540,"elapsed":48,"user":{"displayName":"Tomohiro K","userId":"16906095679101321242"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## ScaledDotProductAttentionクラス\n"],"metadata":{"id":"6cvwgrTiPxJ5"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"_g_QG2-XPxJ8","executionInfo":{"status":"ok","timestamp":1745972169709,"user_tz":-540,"elapsed":9704,"user":{"displayName":"Tomohiro K","userId":"16906095679101321242"}}},"outputs":[],"source":["class ScaledDotProductAttention(nn.Module):\n","    def __init__(self, d_k):\n","        super(ScaledDotProductAttention, self).__init__()\n","        self.scaling_factor = torch.rsqrt(torch.tensor(d_k, dtype=torch.float))\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, Q, K, V, mask=None):\n","        \"\"\"\n","        Args:\n","            Q (Tensor): Queries tensor, shape [batch_size, n_head, seq_len, d_k].\n","            K (Tensor): Keys tensor, shape [batch_size, n_head, seq_len, d_k].\n","            V (Tensor): Values tensor, shape [batch_size, n_head, seq_len, d_v].\n","            mask (Tensor, optional): Mask tensor, shape [batch_size, 1, 1, seq_len].\n","\n","        Returns:\n","            Tensor: Output tensor, shape [batch_size, n_head, seq_len, d_v].\n","            Tensor: Attention weights tensor, shape [batch_size, n_head, seq_len, seq_len].\n","        \"\"\"\n","\n","        # Compute scaled dot-product attention scores\n","        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scaling_factor\n","\n","        if mask is not None:\n","            attn_scores = attn_scores.masked_fill(mask == 1, float('-inf'))\n","\n","        # Compute attention weights\n","        attn_weights = self.softmax(attn_scores)\n","\n","        # Compute weighted sum of values\n","        output = torch.matmul(attn_weights, V)\n","\n","        return output, attn_weights"]},{"cell_type":"markdown","source":["### ScaledDotProductAttentionを使ってみる"],"metadata":{"id":"9sJPXc0hxRKB"}},{"cell_type":"code","source":["# ハイパーパラメータ\n","batch_size = 16\n","n_head = 8\n","seq_len = 10\n","d_k = 64\n","d_v = 128\n","\n","# ScaledDotProductAttentionモジュールのインスタンス化\n","scaled_dot_product_attention = ScaledDotProductAttention(d_k)\n","\n","# ランダムなテンソルを生成\n","Q = torch.randn(batch_size, n_head, seq_len, d_k)\n","K = torch.randn(batch_size, n_head, seq_len, d_k)\n","V = torch.randn(batch_size, n_head, seq_len, d_v)\n","print(f'ランダムQのサイズ: {Q.shape}')\n","print(f'ランダムKのサイズ: {K.shape}')\n","print(f'ランダムVのサイズ: {V.shape}')\n","\n","# マスクの作成\n","# このマスクは、最初の5つの位置だけをアンマスクし、残りの位置をマスクします。\n","mask = torch.ones(batch_size, 1, 1, seq_len)\n","mask[:, :, :, :5] = 0\n","\n","# forwardメソッドを呼び出し\n","output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n","\n","# 出力とAttention weightを表示\n","print(f'出力のサイズ: {output.size()}')  # 出力テンソルのサイズを表示: [batch_size, n_head, seq_len, d_v]\n","print(f'Attention Weightのサイズ: {attn_weights.size()}')  # Attention weightテンソルのサイズを表示: [batch_size, n_head, seq_len, seq_len]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EdrfsTJCwis2","executionInfo":{"status":"ok","timestamp":1745048064600,"user_tz":-540,"elapsed":119,"user":{"displayName":"Tomohiro K","userId":"16906095679101321242"}},"outputId":"18170125-cde5-427b-f77f-b148375957ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ランダムQのサイズ: torch.Size([16, 8, 10, 64])\n","ランダムKのサイズ: torch.Size([16, 8, 10, 64])\n","ランダムVのサイズ: torch.Size([16, 8, 10, 128])\n","出力のサイズ: torch.Size([16, 8, 10, 128])\n","Attention Weightのサイズ: torch.Size([16, 8, 10, 10])\n"]}]},{"cell_type":"markdown","source":["## MultiHeadAttentionクラス"],"metadata":{"id":"ET9qaXSS2m0F"}},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, n_head, d_k, d_v):\n","        super(MultiHeadAttention, self).__init__()\n","        self.n_head = n_head\n","        self.d_k = d_k\n","        self.d_v = d_v\n","\n","        self.W_Q = nn.Linear(d_model, d_k * n_head, bias=False)\n","        self.W_K = nn.Linear(d_model, d_k * n_head, bias=False)\n","        self.W_V = nn.Linear(d_model, d_v * n_head, bias=False)\n","        self.W_O = nn.Linear(d_v * n_head, d_model, bias=False)\n","\n","        self.attention = ScaledDotProductAttention(d_k)\n","\n","    def forward(self, Q, K, V, mask=None):\n","        batch_size = Q.size(0)\n","\n","        # print(f'W_Q直後のサイズ： {self.W_Q(Q).shape}')\n","        # print(f'W_K直後のサイズ： {self.W_K(K).shape}')\n","        # print(f'W_V直後のサイズ： {self.W_V(V).shape}')\n","\n","        # Linear projections\n","        Q = self.W_Q(Q).view(batch_size, -1, self.n_head, self.d_k).transpose(1, 2)\n","        K = self.W_K(K).view(batch_size, -1, self.n_head, self.d_k).transpose(1, 2)\n","        V = self.W_V(V).view(batch_size, -1, self.n_head, self.d_v).transpose(1, 2)\n","        # print(f'Qのサイズ： {Q.shape}')\n","        # print(f'Kのサイズ： {K.shape}')\n","        # print(f'Vのサイズ： {V.shape}')\n","\n","        if mask is not None:\n","            mask = mask.unsqueeze(1)  # [batch_size, 1, seq_len, seq_len]\n","\n","        # Apply Scaled Dot Product Attention\n","        x, attn = self.attention(Q, K, V, mask=mask)  # [batch_size, n_head, seq_len, d_v]\n","\n","        # Concatenate and apply final linear\n","        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.n_head * self.d_v)  # [batch_size, seq_len, n_head * d_v]\n","        output = self.W_O(x)  # [batch_size, seq_len, d_model]\n","\n","        return output, attn"],"metadata":{"id":"QgzkyHNX41X_","executionInfo":{"status":"ok","timestamp":1745972187555,"user_tz":-540,"elapsed":46,"user":{"displayName":"Tomohiro K","userId":"16906095679101321242"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### MultiHeadAttentionを使ってみる"],"metadata":{"id":"tXfk-B1-ybnn"}},{"cell_type":"code","source":["# ハイパーパラメータ\n","batch_size = 16\n","seq_len = 10\n","d_model = 512  # 入力特徴の次元数\n","n_head = 8  # Attention headの数\n","d_k = 64  # キー/クエリベクトルの次元数\n","d_v = 128  # 値ベクトルの次元数\n","\n","# MultiHeadAttentionモジュールのインスタンス化\n","multi_head_attention = MultiHeadAttention(d_model, n_head, d_k, d_v)\n","\n","# ランダムなテンソルを生成\n","Q = torch.randn(batch_size, seq_len, d_model)\n","K = torch.randn(batch_size, seq_len, d_model)\n","V = torch.randn(batch_size, seq_len, d_model)\n","print(f'ランダムQのサイズ: {Q.shape}')\n","print(f'ランダムKのサイズ: {K.shape}')\n","print(f'ランダムVのサイズ: {V.shape}')\n","\n","# オプショナル: マスクの作成\n","# このマスクは、最初の5つの位置だけをアンマスクし、残りの位置をマスクします。\n","mask = torch.ones(batch_size, 1, seq_len)\n","mask[:, :, :5] = 0\n","\n","# forwardメソッドを呼び出し\n","output, attn_weights = multi_head_attention(Q, K, V, mask)\n","\n","# 出力とAttention weightを表示\n","print(f'出力のサイズ: {output.size()}')  # 出力テンソルのサイズを表示: [batch_size, seq_len, d_model]\n","print(f'Attention Weightのサイズ: {attn_weights.size()}')  # Attention weightテンソルのサイズを表示: [batch_size, n_head, seq_len, seq_len]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IC7tYkogylU0","executionInfo":{"status":"ok","timestamp":1745048452265,"user_tz":-540,"elapsed":46,"user":{"displayName":"Tomohiro K","userId":"16906095679101321242"}},"outputId":"79708c77-0c82-49c4-eedb-e5c31f4af6e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ランダムQのサイズ: torch.Size([16, 10, 512])\n","ランダムKのサイズ: torch.Size([16, 10, 512])\n","ランダムVのサイズ: torch.Size([16, 10, 512])\n","W_Q直後のサイズ： torch.Size([16, 10, 512])\n","W_K直後のサイズ： torch.Size([16, 10, 512])\n","W_V直後のサイズ： torch.Size([16, 10, 1024])\n","Qのサイズ： torch.Size([16, 8, 10, 64])\n","Kのサイズ： torch.Size([16, 8, 10, 64])\n","Vのサイズ： torch.Size([16, 8, 10, 128])\n","出力のサイズ: torch.Size([16, 10, 512])\n","Attention Weightのサイズ: torch.Size([16, 8, 10, 10])\n"]}]},{"cell_type":"markdown","source":["## PositionalEncodingクラス\n","Section3で構築した「Positional Encoding」のクラスです。"],"metadata":{"id":"Y8X1KDm5MaP1"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"KEtd-3WwMaP3","executionInfo":{"status":"ok","timestamp":1745972196192,"user_tz":-540,"elapsed":2,"user":{"displayName":"Tomohiro K","userId":"16906095679101321242"}}},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.encoding = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n","        self.encoding[:, 0::2] = torch.sin(position * div_term)\n","        self.encoding[:, 1::2] = torch.cos(position * div_term)\n","        self.encoding = self.encoding.unsqueeze(0)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x (Tensor): Input tensor, shape [batch_size, seq_len, d_model]\n","\n","        Returns:\n","            Tensor: Output tensor, shape [batch_size, seq_len, d_model]\n","        \"\"\"\n","        # Add positional encoding to the input tensor\n","        x = x + self.encoding[:, :x.size(1), :]\n","        return x"]},{"cell_type":"markdown","source":["### PositionalEncodingを使ってみる"],"metadata":{"id":"UOFr4uNp6etz"}},{"cell_type":"code","source":["# ハイパーパラメータの設定\n","batch_size = 16\n","seq_len = 10\n","d_model = 512  # モデルの次元数\n","\n","# PositionalEncodingモジュールのインスタンス化\n","positional_encoding = PositionalEncoding(d_model)\n","\n","# ランダムなテンソルを生成\n","x = torch.randn(batch_size, seq_len, d_model)\n","\n","# forwardメソッドを呼び出し\n","encoded_x = positional_encoding(x)\n","\n","# エンコードされたデータの表示\n","print(encoded_x.size())  # エンコードされたテンソルのサイズを表示: [batch_size, seq_len, d_model]\n","\n","# エンコードされたポジショナルエンコーディングをNumPy配列に変換\n","print(encoded_x.size())\n","print(encoded_x.squeeze(0).shape)\n","encoded_x_0 = encoded_x[0].squeeze(0).detach().numpy()\n","print(encoded_x_0.shape)\n","# グラフをプロット\n","plt.figure(figsize=(10, 10))\n","plt.pcolormesh(encoded_x_0.transpose(), cmap='viridis')\n","plt.xlabel('Sequence Position')\n","plt.ylabel('Embedding Dimension')\n","plt.colorbar()\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pJpSFXoA6kwP","executionInfo":{"status":"ok","timestamp":1745735983312,"user_tz":-540,"elapsed":72,"user":{"displayName":"Tomohiro K","userId":"16906095679101321242"}},"outputId":"9824d91d-fe79-49f3-877c-037c6762d573"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([16, 10, 512])\n"]}]},{"cell_type":"markdown","source":["## PositionwiseFeedForwardクラス"],"metadata":{"id":"P8AnHqVJgCE2"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"_6JcaTaygCE3","executionInfo":{"status":"ok","timestamp":1745972225659,"user_tz":-540,"elapsed":1,"user":{"displayName":"Tomohiro K","userId":"16906095679101321242"}}},"outputs":[],"source":["class PositionwiseFeedForward(nn.Module):\n","    def __init__(self, d_model, d_ff, dropout=0.1):\n","        \"\"\"\n","        Args:\n","            d_model (int): The dimension of the model (also the input and output dimension).\n","            d_ff (int): The dimension of the feed-forward hidden layer.\n","            dropout (float): Dropout probability.\n","        \"\"\"\n","        super(PositionwiseFeedForward, self).__init__()\n","        self.w_1 = nn.Linear(d_model, d_ff)\n","        self.w_2 = nn.Linear(d_ff, d_model)\n","        self.dropout = nn.Dropout(dropout)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x (Tensor): Input tensor, shape [batch_size, seq_len, d_model]\n","\n","        Returns:\n","            Tensor: Output tensor, shape [batch_size, seq_len, d_model]\n","        \"\"\"\n","        return self.w_2(self.dropout(self.relu(self.w_1(x))))"]},{"cell_type":"markdown","source":["### PositionwiseFeedForwardを使ってみる"],"metadata":{"id":"zyQ3x3UR9yOu"}},{"cell_type":"code","source":["# ハイパーパラメータの設定\n","batch_size = 16\n","seq_len = 10\n","d_model = 512  # モデルの次元数\n","d_ff = 2048  # フィードフォワード隠れ層の次元数\n","\n","# PositionwiseFeedForwardモジュールのインスタンス化\n","positionwise_ff = PositionwiseFeedForward(d_model, d_ff)\n","\n","# ランダムなテンソルを生成\n","x = torch.randn(batch_size, seq_len, d_model)\n","\n","# forwardメソッドを呼び出し\n","output = positionwise_ff(x)\n","\n","# 出力の表示\n","print(output.size())  # 出力テンソルのサイズを表示: [batch_size, seq_len, d_model]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WQKsrN3P94fV","executionInfo":{"status":"ok","timestamp":1745735999510,"user_tz":-540,"elapsed":37,"user":{"displayName":"Tomohiro K","userId":"16906095679101321242"}},"outputId":"0436297e-6242-47a2-f096-3b2c3473102d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([16, 10, 512])\n"]}]},{"cell_type":"markdown","source":["## Token Embeddingを使ってみる\n","Token Embeddingは、PyTorchの`nn.Embedding()`を使いを実装することができます。  \n","`nn.Embedding()`は通常、単語の埋め込み（単語ベクトル）を学習するために自然言語処理タスクで使用されます。  \n","埋め込みベクトルのテンソルの形状は[5, 50]となり、5つの単語それぞれに対して50次元の埋め込みベクトルが得られています。  \n","訓練中にこれらの埋め込みベクトルはバックプロパゲーションを使って更新されます。  \n","これにより、タスクの性能が次第に向上します。  "],"metadata":{"id":"W_cHw-8H5Xj4"}},{"cell_type":"code","source":["# ハイパーパラメータの設定\n","vocab_size = 10  # 語彙サイズ\n","embed_dim = 50   # 埋め込みベクトルの次元数\n","\n","# nn.Embeddingのインスタンスを作成\n","embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n","\n","# 入力データ（単語IDのテンソル）\n","input_data = torch.tensor([0, 1, 2, 3, 4], dtype=torch.long)  # 単語IDの例\n","\n","# 埋め込み層を通じて入力データを変換\n","embedded_data = embedding_layer(input_data)\n","\n","# 埋め込まれたデータの表示\n","print(embedded_data.shape)\n","print(embedded_data[0])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3XKyrYdt5ZEP","executionInfo":{"status":"ok","timestamp":1745737268409,"user_tz":-540,"elapsed":59,"user":{"displayName":"Tomohiro K","userId":"16906095679101321242"}},"outputId":"6b53f611-84fd-406b-8363-c673f9c509ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 50])\n","tensor([ 0.6330, -0.0159, -1.8124,  1.3223, -0.0433, -1.6019,  1.0942, -1.6316,\n","        -0.4016, -0.0567, -0.0388,  0.7298, -0.5838, -1.6775,  0.3801, -0.5028,\n","        -1.2791,  0.0763, -1.4816,  0.6320,  0.3060,  0.8212, -0.3067, -0.4664,\n","        -0.2831, -1.4834,  0.5508,  1.7823,  0.3094,  0.5186,  1.1456, -0.1017,\n","         1.0952, -0.9636,  0.7506,  0.2427, -0.2452,  0.6169,  1.3137,  0.6934,\n","        -0.1556,  1.1358, -0.6221, -1.4098,  0.1731, -1.1881,  0.7409,  0.8279,\n","         1.0083,  0.2723], grad_fn=<SelectBackward0>)\n"]}]},{"cell_type":"markdown","source":["## Layer Normalizationを使ってみる"],"metadata":{"id":"5g00_STZ_Rr7"}},{"cell_type":"code","source":["# ハイパーパラメータの設定\n","d_model = 512  # モデルの次元数\n","eps = 1e-6  # イプシロン（数値安定性のため）\n","\n","# LayerNormのインスタンス化\n","layer_norm = nn.LayerNorm(normalized_shape=d_model, eps=eps)\n","\n","# ランダムなテンソルを生成\n","batch_size = 16\n","seq_len = 10\n","x = torch.randn(batch_size, seq_len, d_model)\n","\n","# LayerNormを適用\n","normalized_x = layer_norm(x)\n","\n","# 結果の表示\n","print(normalized_x.size())  # 出力テンソルのサイズを表示: [batch_size, seq_len, d_model]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IDzPeaMK54Ui","executionInfo":{"status":"ok","timestamp":1745736276768,"user_tz":-540,"elapsed":43,"user":{"displayName":"Tomohiro K","userId":"16906095679101321242"}},"outputId":"c37c6d08-86ba-43bb-d2d6-959a5e9f9ea2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([16, 10, 512])\n"]}]},{"cell_type":"markdown","source":["# 要素を使ってTransformerを構築"],"metadata":{"id":"CW7y7UBXAlnO"}},{"cell_type":"markdown","source":["## EncoderLayerクラス"],"metadata":{"id":"hs-EOPaKnf9A"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"W2vttcSpLchW","executionInfo":{"status":"ok","timestamp":1745972868939,"user_tz":-540,"elapsed":37,"user":{"displayName":"Tomohiro K","userId":"16906095679101321242"}}},"outputs":[],"source":["class EncoderLayer(nn.Module):\n","    def __init__(self, d_model, n_head, d_k, d_v, d_ff, dropout=0.1):\n","        super(EncoderLayer, self).__init__()\n","        self.self_attn = MultiHeadAttention(d_model, n_head, d_k, d_v)\n","        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n","        self.layer_norm1 = nn.LayerNorm(d_model)\n","        self.layer_norm2 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask):\n","        # Self-attention sublayer\n","        attn_output, _ = self.self_attn(x, x, x, mask)\n","        x = x + self.dropout(attn_output) # 迂回してきたものと足し合わせる\n","        x = self.layer_norm1(x)\n","\n","        # Feed-forward sublayer\n","        ff_output = self.feed_forward(x)\n","        x = x + self.dropout(ff_output) # 迂回してきたものと足し合わせる\n","        x = self.layer_norm2(x)\n","\n","        return x"]},{"cell_type":"markdown","source":["## Encoderクラス"],"metadata":{"id":"i8IP7alygB5P"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"93FBOx15gB5R","executionInfo":{"status":"ok","timestamp":1745972871285,"user_tz":-540,"elapsed":42,"user":{"displayName":"Tomohiro K","userId":"16906095679101321242"}}},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, d_model, n_head, d_k, d_v, d_ff, num_layers, dropout=0.1):\n","        super(Encoder, self).__init__()\n","        # 層の数だけ繰り返すためのリストを作成\n","        self.layers = nn.ModuleList([EncoderLayer(d_model, n_head, d_k, d_v, d_ff, dropout) for _ in range(num_layers)])\n","        self.layer_norm = nn.LayerNorm(d_model)\n","\n","    def forward(self, x, mask):\n","        for layer in self.layers:\n","            x = layer(x, mask)\n","        x = self.layer_norm(x)\n","        return x"]},{"cell_type":"markdown","source":["### Encoderを使ってみる"],"metadata":{"id":"q1JUGi6oBk5W"}},{"cell_type":"code","source":["# パラメータ設定\n","d_model = 512  # 埋め込みの次元数\n","n_head = 8     # アテンションヘッドの数\n","d_k = d_v = 64 # キーと値の次元数\n","d_ff = 2048    # フィードフォワードネットワークの内部次元数\n","num_layers = 6 # エンコーダの層の数\n","batch_size = 32  # バッチサイズ\n","seq_len = 100  # 入力シーケンスの長さ\n","dropout = 0.1  # ドロップアウト率\n","\n","# エンコーダのインスタンス化\n","encoder = Encoder(d_model, n_head, d_k, d_v, d_ff, num_layers, dropout)\n","\n","# モデルを評価モードに設定（ドロップアウトなどが無効になる）\n","encoder.eval()\n","\n","# ダミーの入力データとマスクの生成\n","input_tensor = torch.rand(batch_size, seq_len, d_model)  # ダミーの入力テンソル\n","mask = torch.zeros(batch_size, seq_len, seq_len)  # マスク（ここでは全てマスクなし）\n","\n","# エンコーダに入力データを通す\n","with torch.no_grad():  # 勾配計算を行わない\n","    encoded_output = encoder(input_tensor, mask)\n","\n","print(\"Encoded output shape:\", encoded_output.shape)  # 出力テンソルの形状を表示: [batch_size, seq_len, d_model]\n","print(\"Encoded output:\", encoded_output[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vi2fzfuBBoJG","executionInfo":{"status":"ok","timestamp":1745972895963,"user_tz":-540,"elapsed":3230,"user":{"displayName":"Tomohiro K","userId":"16906095679101321242"}},"outputId":"a65e1686-a383-4745-df79-7d55924814e2"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Encoded output shape: torch.Size([32, 100, 512])\n","Encoded output: tensor([[-0.2766, -0.2822,  0.8418,  ..., -1.8167,  0.0326,  1.2456],\n","        [ 0.8136, -0.4500, -0.4342,  ...,  0.2924,  0.0264,  0.6845],\n","        [ 1.7041, -1.0821, -1.0971,  ...,  0.0374,  0.1104,  1.4511],\n","        ...,\n","        [ 1.9274,  0.4755, -0.0206,  ..., -2.0815, -0.7704, -0.1266],\n","        [ 2.0487, -1.3530,  1.4767,  ...,  0.1671, -0.9328,  0.7408],\n","        [ 0.6506, -1.2212,  0.6088,  ..., -0.3580, -0.9301,  0.1227]])\n"]}]},{"cell_type":"markdown","source":["## DecoderLayerクラス"],"metadata":{"id":"ggq_2Si9NpYK"}},{"cell_type":"code","execution_count":9,"metadata":{"id":"5kBmoNecNpYL","executionInfo":{"status":"ok","timestamp":1745973139089,"user_tz":-540,"elapsed":11,"user":{"displayName":"Tomohiro K","userId":"16906095679101321242"}}},"outputs":[],"source":["class DecoderLayer(nn.Module):\n","    def __init__(self, d_model, n_head, d_k, d_v, d_ff, dropout=0.1):\n","        super(DecoderLayer, self).__init__()\n","        self.self_attn = MultiHeadAttention(d_model, n_head, d_k, d_v)\n","        self.enc_dec_attn = MultiHeadAttention(d_model, n_head, d_k, d_v)\n","        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n","        self.layer_norm1 = nn.LayerNorm(d_model)\n","        self.layer_norm2 = nn.LayerNorm(d_model)\n","        self.layer_norm3 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, enc_output, src_mask, tgt_mask):\n","        # Self-attention sublayer with target mask\n","        self_attn_output, _ = self.self_attn(x, x, x, tgt_mask)\n","        x = x + self.dropout(self_attn_output)\n","        x = self.layer_norm1(x)\n","\n","        # Encoder-decoder attention sublayer with source mask\n","        enc_dec_attn_output, _ = self.enc_dec_attn(x, enc_output, enc_output, src_mask)\n","        x = x + self.dropout(enc_dec_attn_output)\n","        x = self.layer_norm2(x)\n","\n","        # Feed-forward sublayer\n","        ff_output = self.feed_forward(x)\n","        x = x + self.dropout(ff_output)\n","        x = self.layer_norm3(x)\n","\n","        return x"]},{"cell_type":"markdown","source":["## Decoderクラス"],"metadata":{"id":"57r3wOOnNpYL"}},{"cell_type":"code","execution_count":10,"metadata":{"id":"Mk8klJbdNpYM","executionInfo":{"status":"ok","timestamp":1745973495120,"user_tz":-540,"elapsed":2,"user":{"displayName":"Tomohiro K","userId":"16906095679101321242"}}},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, d_model, n_head, d_k, d_v, d_ff, num_layers, dropout=0.1):\n","        super(Decoder, self).__init__()\n","        # 層の数だけ繰り返すためのリストを作成\n","        self.layers = nn.ModuleList([DecoderLayer(d_model, n_head, d_k, d_v, d_ff, dropout) for _ in range(num_layers)])\n","        self.layer_norm = nn.LayerNorm(d_model)\n","\n","    def forward(self, x, enc_output, src_mask, tgt_mask):\n","        for layer in self.layers:\n","            x = layer(x, enc_output, src_mask, tgt_mask)\n","        x = self.layer_norm(x)\n","        return x"]},{"cell_type":"markdown","source":["### Dcoderを使ってみる"],"metadata":{"id":"tNcmyOGfCp4V"}},{"cell_type":"code","source":["# パラメータ設定\n","d_model = 512  # 埋め込みの次元数\n","n_head = 8     # アテンションヘッドの数\n","d_k = d_v = 64 # キーと値の次元数\n","d_ff = 2048    # フィードフォワードネットワークの内部次元数\n","num_layers = 6 # デコーダの層の数\n","batch_size = 32  # バッチサイズ\n","tgt_seq_len = 100  # デコーダへの入力のシーケンスの長さ\n","src_seq_len = 120  # エンコーダからの出力のシーケンスの長さ\n","dropout = 0.1  # ドロップアウト率\n","\n","# デコーダのインスタンス化\n","decoder = Decoder(d_model, n_head, d_k, d_v, d_ff, num_layers, dropout)\n","\n","# モデルを評価モードに設定（ドロップアウトなどが無効になる）\n","decoder.eval()\n","\n","# ダミーの入力\n","input_tensor = torch.rand(batch_size, tgt_seq_len, d_model)  # デコーダへの入力\n","enc_output = torch.rand(batch_size, src_seq_len, d_model)    # エンコーダからの出力\n","\n","# マスクの設定\n","src_mask = torch.zeros(batch_size, tgt_seq_len, src_seq_len)\n","tgt_mask = torch.triu(torch.ones(batch_size, tgt_seq_len, tgt_seq_len), diagonal=1)  # 一部を0に\n","\n","# デコーダにデータを通す\n","with torch.no_grad():  # 勾配計算を行わない\n","    decoded_output = decoder(input_tensor, enc_output, src_mask, tgt_mask)\n","\n","print(\"Decoded output shape:\", decoded_output.shape)  # 出力テンソルの形状を表示: [batch_size, tgt_seq_len, d_model]\n","print(\"Decoded output:\", decoded_output[0])"],"metadata":{"id":"eIGB9nyYCtIs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745973502688,"user_tz":-540,"elapsed":3320,"user":{"displayName":"Tomohiro K","userId":"16906095679101321242"}},"outputId":"1bc405c7-48ac-40c9-cbcd-5117ed277ddc"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Decoded output shape: torch.Size([32, 100, 512])\n","Decoded output: tensor([[ 0.3445,  0.2929,  0.7127,  ..., -0.0193,  1.6462, -1.7966],\n","        [-1.1351, -0.1350,  1.6325,  ..., -0.1812,  0.9246, -1.1975],\n","        [-0.5609,  0.8292, -0.7849,  ..., -1.0452,  0.1029, -0.9809],\n","        ...,\n","        [-0.3113,  0.7378,  0.5019,  ..., -1.8323,  2.0693,  0.9052],\n","        [ 0.8796, -0.8885,  1.6949,  ..., -0.4657,  1.9662,  0.6901],\n","        [ 0.0764,  0.5260, -0.1914,  ...,  0.9847,  1.0884,  1.0874]])\n"]}]},{"cell_type":"markdown","source":["## Transformerクラス"],"metadata":{"id":"qZ1A2OkSN6oV"}},{"cell_type":"code","execution_count":12,"metadata":{"id":"Lu5ydP0ON6oV","executionInfo":{"status":"ok","timestamp":1745973961706,"user_tz":-540,"elapsed":1,"user":{"displayName":"Tomohiro K","userId":"16906095679101321242"}}},"outputs":[],"source":["class Transformer(nn.Module):\n","    def __init__(self, d_model, n_head, d_k, d_v, d_ff, num_encoder_layers, num_decoder_layers,\n","                 src_vocab_size, tgt_vocab_size, max_src_seq_len, max_tgt_seq_len, dropout=0.1):\n","        super(Transformer, self).__init__()\n","\n","        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n","        self.pos_encoder = PositionalEncoding(d_model, max_src_seq_len)\n","        self.encoder = Encoder(d_model, n_head, d_k, d_v, d_ff, num_encoder_layers, dropout)\n","\n","        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n","        self.pos_decoder = PositionalEncoding(d_model, max_tgt_seq_len)\n","        self.decoder = Decoder(d_model, n_head, d_k, d_v, d_ff, num_decoder_layers, dropout)\n","\n","        self.output_linear = nn.Linear(d_model, tgt_vocab_size)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","        self.src_vocab_size = src_vocab_size\n","        self.tgt_vocab_size = tgt_vocab_size\n","        self.d_model = d_model\n","\n","    def forward(self, src, tgt, src_mask, tgt_mask):\n","        src_emb = self.encoder_embedding(src) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float))\n","        src_emb = self.pos_encoder(src_emb)\n","        enc_output = self.encoder(src_emb, src_mask)\n","\n","        tgt_emb = self.decoder_embedding(tgt) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float))\n","        tgt_emb = self.pos_decoder(tgt_emb)\n","        dec_output = self.decoder(tgt_emb, enc_output, src_mask, tgt_mask)\n","\n","        output = self.output_linear(dec_output)\n","        return self.softmax(output)"]},{"cell_type":"markdown","source":["## Transformerの訓練をやってみる\n","簡単な翻訳データセットを作成し、そのデータセットを使用してモデルを訓練します。  \n","使用する翻訳データセットは非常に小さく、実際の翻訳タスクを学習するには不十分ですが、デモンストレーションのためには十分です。\n"],"metadata":{"id":"XeGiQCcIoZ6V"}},{"cell_type":"code","source":["# 簡単な翻訳データセットを作成する\n","class TranslationDataset(Dataset):\n","    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab):\n","        self.src_sentences = src_sentences\n","        self.tgt_sentences = tgt_sentences\n","        self.src_vocab = src_vocab\n","        self.tgt_vocab = tgt_vocab\n","\n","    def __len__(self):\n","        # 文章の長さを取得\n","        return len(self.src_sentences)\n","\n","    def __getitem__(self, idx):\n","        # データローダーでデータを取り出す\n","        # インデックスに対応した要素\n","        src_sentence = self.src_sentences[idx]\n","        tgt_sentence = self.tgt_sentences[idx]\n","        # それぞれの単語に対応したID\n","        src_indices = [self.src_vocab[token] for token in src_sentence.split()]\n","        tgt_indices = [self.tgt_vocab[token] for token in tgt_sentence.split()]\n","        return torch.tensor(src_indices), torch.tensor(tgt_indices)\n","\n","# 簡単な語彙とデータセットを作成\n","src_vocab = {'hello': 0, 'world': 1, 'EOS': 2}\n","tgt_vocab = {'こんにちは': 0, '世界': 1, 'EOS': 2}\n","src_sentences = ['hello world EOS', 'world hello EOS']\n","tgt_sentences = ['こんにちは 世界 EOS', '世界 こんにちは EOS']\n","\n","# データセットとデータローダーの作成\n","dataset = TranslationDataset(src_sentences, tgt_sentences, src_vocab, tgt_vocab)\n","dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n","\n","# モデルのインスタンス化\n","transformer = Transformer(\n","    d_model=512, n_head=8, d_k=64, d_v=64, d_ff=2048,\n","    num_encoder_layers=3, num_decoder_layers=3,\n","    src_vocab_size=len(src_vocab), tgt_vocab_size=len(tgt_vocab),\n","    max_src_seq_len=10, max_tgt_seq_len=10, dropout=0.1\n",")\n","\n","# 損失関数と最適化手法\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(transformer.parameters(), lr=0.001)\n","\n","# 訓練ループ\n","for epoch in range(10):  # 実際にはもっと多くのエポックが必要です\n","    transformer.train()\n","    total_loss = 0\n","    for src, tgt in dataloader:\n","        # モデルの出力を計算\n","        output = transformer(src, tgt[:, :-1], None, None)\n","\n","        # 損失を計算\n","        loss = criterion(output.view(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n","\n","        # 勾配を初期化\n","        optimizer.zero_grad()\n","\n","        # バックプロパゲーション\n","        loss.backward()\n","\n","        # パラメータの更新\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    print(f'Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}')"],"metadata":{"id":"bQd7Ekwif4Ty","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745974054936,"user_tz":-540,"elapsed":8546,"user":{"displayName":"Tomohiro K","userId":"16906095679101321242"}},"outputId":"ca6f2808-2dc2-4cf7-b9d6-7d03c6c093c2"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 1.0434155464172363\n","Epoch 2, Loss: 1.0512516498565674\n","Epoch 3, Loss: 1.0508849620819092\n","Epoch 4, Loss: 1.0244863033294678\n","Epoch 5, Loss: 1.2967438697814941\n","Epoch 6, Loss: 1.2894434928894043\n","Epoch 7, Loss: 1.0014259815216064\n","Epoch 8, Loss: 1.051102876663208\n","Epoch 9, Loss: 1.05092191696167\n","Epoch 10, Loss: 1.0528696775436401\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"z3yas322_ZnW"},"execution_count":null,"outputs":[]}]}