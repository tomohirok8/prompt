{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# テキストデータの入手・変数への格納\n",
        "\n",
        "  * 夏目漱石の「吾輩は猫である」（青空文庫）を利用\n",
        "  * requestによりWebアクセス\n",
        "\n",
        "  * bs4(Beautiful Soup)のHTML parserを利用して解析"
      ],
      "metadata": {
        "id": "aDNE_HtUqJRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, bs4"
      ],
      "metadata": {
        "id": "Crmb9dQeqNHT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = requests.get(\"https://www.aozora.gr.jp/cards/000148/files/789_14547.html\")"
      ],
      "metadata": {
        "id": "e_TpoDv1rLkL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = bs4.BeautifulSoup(response.content, \"html.parser\")"
      ],
      "metadata": {
        "id": "Oi1RsTWorXbJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elems = soup.select(\".main_text\")"
      ],
      "metadata": {
        "id": "qVjYmyR4rjGq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(elems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W95eQMVEszWO",
        "outputId": "ad4dbc49-926d-4155-e3c1-0347f690ffb3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "bs4.element.ResultSet"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = ''.join([elm.text for elm in elems])"
      ],
      "metadata": {
        "id": "zAOti1NBruFJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "W0dGbYXssfAh",
        "outputId": "8eb6eec2-fcee-4e99-a200-decff66f10a8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n一\\n\\r\\n\\u3000吾輩は猫である。名前はまだ無い。\\r\\n\\u3000どこで生れたかとんと見当がつかぬ。何でも薄暗いじめじめした所でニャーニャー泣いていた事だけは記憶している。吾輩はここで始めて人間というものを見た。しかもあとで聞くとそれは書生という人間中で一番獰悪な種族であったそうだ。この書生というのは時々我々を捕えて煮て食うという話である。しかしその当時は何という考もなかったから別段恐しいとも思わなかった。ただ彼の掌に載せられてスーと持ち上げられた時何だかフワフワした感じがあったばかりである。掌の上で少し落ちついて書生の顔を見たのがいわゆる人間というものの見始であろう。この時妙なものだと思った感じが今でも残っている。第一毛をもって装飾されべきはずの顔がつるつるしてまるで薬缶だ。その後猫にもだいぶ逢ったがこんな片輪には一度も出会わした事がない。のみならず顔の真中があまりに突起している。そうしてその穴の中から時々ぷうぷうと煙を吹く。どうも咽せぽくて実に弱った。これが人間の飲む煙草というものである事はようやくこの頃知った。\\r\\n\\u3000この書生の掌の裏でしばらくはよい心持に坐っておったが、しばらくすると非常な速力で運転し始めた。書生が動くのか自分だけが動くのか分らないが無暗に眼が廻る。胸が悪くなる。到底助からないと思っていると、どさりと音がして眼から火が出た。それまでは記憶しているがあとは何の事やらいくら考え出そうとしても分らない。\\r\\n\\u3000ふと気が付いて見ると書生はいない。たくさんおった兄弟が一疋も見えぬ。肝心の母親さえ姿を隠してしまった。その上今までの所とは違って無暗に明るい。眼を明いていられぬくらいだ。はてな何でも容子がおかしいと、のそのそ這い出して見ると非常に痛い。吾輩は藁の上から急に笹原の中へ棄てられたのである。\\r\\n\\u3000ようやくの思いで笹原を這い出すと向うに大きな池がある。吾輩は池の前に坐ってどうしたらよかろうと考えて見た。別にこれという分別も出ない。しばらくして泣いたら書生がまた迎に来てくれるかと考え付いた。ニャー、ニャーと試みにやって見たが誰も来ない。そのうち池の上をさらさらと風が渡って日が暮れかかる。腹が非常に減って来た。泣きたくても声が出ない。仕方がない、何でもよいから食物のある所まであるこうと決心をしてそろりそろりと池を左りに廻り始めた。どうも非常に苦しい。そこを我慢して無理やりに'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 前処理\n",
        "\n",
        "* reを利用し、改行コード（\\n一\\n, \\r\\n, \\n)と\\u3000(全角スペース)の削除\n",
        "* janomeを利用し、すべてカタカナ変換する\n",
        "* jaconvを利用し、カタカナ→ひらがな変換する\n"
      ],
      "metadata": {
        "id": "KXD86ciRxCGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install janome\n",
        "!pip install jaconv"
      ],
      "metadata": {
        "id": "WDLtLp9AyZzC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c67af80b-d2e5-46a9-b567-8138f7454cfb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting janome\n",
            "  Downloading Janome-0.5.0-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: janome\n",
            "Successfully installed janome-0.5.0\n",
            "Collecting jaconv\n",
            "  Downloading jaconv-0.3.4.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: jaconv\n",
            "  Building wheel for jaconv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jaconv: filename=jaconv-0.3.4-py3-none-any.whl size=16416 sha256=033dfaabdd1b21ea583699cb26b75c4c59d6cb0806d87aa37a28fc4e69149e9d\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/8f/2e/a730bf1fca05b33e532d5d91dabdf406c9b718ec85b01b1b54\n",
            "Successfully built jaconv\n",
            "Installing collected packages: jaconv\n",
            "Successfully installed jaconv-0.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "start_idx = int(len(texts) * random.random())\n",
        "end_idx = start_idx + 1000\n",
        "\n",
        "texts[start_idx:end_idx]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "6TY1p60yvv0A",
        "outputId": "93a1cc3e-8ce6-4a2e-bc5e-f7deaa671130"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'、いわんや肩をやと、一つ、こう行くかな」\\r\\n「そうおいでになったと、よろしい。薫風南より来って、殿閣微涼を生ず。こう、ついでおけば大丈夫なものだ」\\r\\n「おや、ついだのは、さすがにえらい。まさか、つぐ気遣はなかろうと思った。ついで、くりゃるな八幡鐘をと、こうやったら、どうするかね」\\r\\n「どうするも、こうするもないさ。一剣天に倚って寒し――ええ、面倒だ。思い切って、切ってしまえ」\\r\\n「やや、大変大変。そこを切られちゃ死んでしまう。おい冗談じゃない。ちょっと待った」\\r\\n「それだから、さっきから云わん事じゃない。こうなってるところへは這入れるものじゃないんだ」\\r\\n「這入って失敬仕り候。ちょっとこの白をとってくれたまえ」\\r\\n「それも待つのかい」\\r\\n「ついでにその隣りのも引き揚げて見てくれたまえ」\\r\\n「ずうずうしいぜ、おい」\\r\\n「Do you see the boy か。――なに君と僕の間柄じゃないか。そんな水臭い事を言わずに、引き揚げてくれたまえな。死ぬか生きるかと云う場合だ。しばらく、しばらくって花道から馳け出してくるところだよ」\\r\\n「そんな事は僕は知らんよ」\\r\\n「知らなくってもいいから、ちょっとどけたまえ」\\r\\n「君さっきから、六返待ったをしたじゃないか」\\r\\n「記憶のいい男だな。向後は旧に倍し待ったを仕り候。だからちょっとどけたまえと云うのだあね。君もよッぽど強情だね。座禅なんかしたら、もう少し捌けそうなものだ」\\r\\n「しかしこの石でも殺さなければ、僕の方は少し負けになりそうだから……」\\r\\n「君は最初から負けても構わない流じゃないか」\\r\\n「僕は負けても構わないが、君には勝たしたくない」\\r\\n「飛んだ悟道だ。相変らず春風影裏に電光をきってるね」\\r\\n「春風影裏じゃない、電光影裏だよ。君のは逆だ」\\r\\n「ハハハハもうたいてい逆かになっていい時分だと思ったら、やはりたしかなところがあるね。それじゃ仕方がないあきらめるかな」\\r\\n「生死事大、無常迅速、あきらめるさ」\\r\\n「アーメン」と迷亭先生今度はまるで関係のない方面へぴしゃりと一石を下した。\\r\\n\\u3000床の間の前で迷亭君と独仙君が一生懸命に輸贏を争っていると、座敷の入口には、寒月君と東風君が相ならんでその傍に主人が黄色い顔をして坐っている。寒月君の前に鰹節が三本、裸のまま畳の上に行儀よく排列してあるのは奇観である。\\r\\n\\u3000この鰹節の出処は寒月君の懐'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "VVtbftH3v8Qm",
        "outputId": "cff98867-af85-4792-97c8-2e04e72ec782"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n一\\n\\r\\n\\u3000吾輩は猫である。名前はまだ無い。\\r\\n\\u3000どこで生れたかとんと見当がつかぬ。何でも薄暗いじめじめした所でニャーニャー泣いていた事だけは記憶している。吾輩はここで始めて人間というものを見た。しかもあとで聞くとそれは書生という人間中で一番獰悪な種族であったそうだ。この書生というのは時々我々を捕えて煮て食うという話である。しかしその当時は何という考もなかったから別段恐しいとも思わなかった。ただ彼の掌に載せられてスーと持ち上げられた時何だかフワフワした感じがあったばかりである。掌の上で少し落ちついて書生の顔を見たのがいわゆる人間というものの見始であろう。この時妙なものだと思った感じが今でも残っている。第一毛をもって装飾されべきはずの顔がつるつるしてまるで薬缶だ。その後猫にもだいぶ逢ったがこんな片輪には一度も出会わした事がない。のみならず顔の真中があまりに突起している。そうしてその穴の中から時々ぷうぷうと煙を吹く。どうも咽せぽくて実に弱った。これが人間の飲む煙草というものである事はようやくこの頃知った。\\r\\n\\u3000この書生の掌の裏でしばらくはよい心持に坐っておったが、しばらくすると非常な速力で運転し始めた。書生が動くのか自分だけが動くのか分らないが無暗に眼が廻る。胸が悪くなる。到底助からないと思っていると、どさりと音がして眼から火が出た。それまでは記憶しているがあとは何の事やらいくら考え出そうとしても分らない。\\r\\n\\u3000ふと気が付いて見ると書生はいない。たくさんおった兄弟が一疋も見えぬ。肝心の母親さえ姿を隠してしまった。その上今までの所とは違って無暗に明るい。眼を明いていられぬくらいだ。はてな何でも容子がおかしいと、のそのそ這い出して見ると非常に痛い。吾輩は藁の上から急に笹原の中へ棄てられたのである。\\r\\n\\u3000ようやくの思いで笹原を這い出すと向うに大きな池がある。吾輩は池の前に坐ってどうしたらよかろうと考えて見た。別にこれという分別も出ない。しばらくして泣いたら書生がまた迎に来てくれるかと考え付いた。ニャー、ニャーと試みにやって見たが誰も来ない。そのうち池の上をさらさらと風が渡って日が暮れかかる。腹が非常に減って来た。泣きたくても声が出ない。仕方がない、何でもよいから食物のある所まであるこうと決心をしてそろりそろりと池を左りに廻り始めた。どうも非常に苦しい。そこを我慢して無理やりに'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "pattern = re.compile(r\"\\n一\\n|\\r\\n|\\n|\\u3000\", re.MULTILINE | re.DOTALL)\n",
        "\n",
        "texts = re.sub(pattern, \"\", texts)"
      ],
      "metadata": {
        "id": "m9dAXeBiwxiV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from janome.tokenizer import Tokenizer\n",
        "import jaconv\n",
        "\n",
        "def get_yomigana(original_txt):\n",
        "  t = Tokenizer()\n",
        "  l = []\n",
        "  for w in t.tokenize(original_txt):\n",
        "    reading = w.reading\n",
        "    if  reading == '*':\n",
        "      l.append(w.base_form)\n",
        "    else:\n",
        "      l.append(reading)\n",
        "  hira = jaconv.kata2hira(''.join(l))\n",
        "  return hira"
      ],
      "metadata": {
        "id": "_lqK2FV53B7F"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_yomigana(\"現在、気温が40度近くです。\")"
      ],
      "metadata": {
        "id": "Xo941m3t3DqJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "59d7e073-28f7-4dc4-bf8d-6eb2a061e032"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'げんざい、きおんが40どちかくです。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lyrics = get_yomigana(\"\"\"Yo! 私たちはライジングボルテッカーズ\n",
        "これからどんなポケモンに会えるのかな\n",
        "冒険が楽しみだ\n",
        "\n",
        "\n",
        "Yo! Ready Go!\n",
        "それならもっとポケモン　学んでみよう\n",
        "まずはドンがつくポケモン！\n",
        "3 2 1 Let’s Go!\n",
        "\n",
        "\n",
        "コライドン　タンドン　ウオチルドン\n",
        "ミライドン　ドンカラス　トリトドン\n",
        "次はリコと一緒に？\n",
        "Are You Ready?\n",
        "\n",
        "\n",
        "ん〜　やれるかな？　一緒にやってみよう！\n",
        "コライドン　タンドン　ウオチルドン\n",
        "ミライドン　ドンカラス　トリトドン\n",
        "\n",
        "\n",
        "Brave in Heart Heart Heart\n",
        "空の上で　なみのり\n",
        "ドキドキ　ワクワクな\n",
        "冒険の始まり\n",
        "舵を取り　世界へ続く道\n",
        "仲間の証のハンドサイン\n",
        "みんなで　ライジングボルテッカーズ\n",
        "\n",
        "\n",
        "もっともっと学んでいこー！\n",
        "次はリージョンフォームいるポケモン！\n",
        "ロイにお手本　見せてもらおう！\n",
        "3 2 1 Go!\n",
        "\n",
        "\n",
        "コラッタ　ラッタ　ガラガラ\n",
        "ダルマッカ　ビリリダマ　落下したら　だいばくはつ\n",
        "次はこの人がやってくれるよ\n",
        "Let’s Go!\n",
        "\n",
        "\n",
        "フリードだ！おれに任せとけ！！\n",
        "コラッタ　ラッタ　ガラガラ\n",
        "ダルマッカ　ビリリダマ　落下したら　だいばくはつ\n",
        "\n",
        "\n",
        "Brave Bird Bird Bird\n",
        "勇気と共に空へ\n",
        "Fly in the Sky Sky Sky\n",
        "目指すよ　あの場所へ\n",
        "傷つき　たまに道　転んでも\n",
        "仲間の証のハンドサイン\n",
        "みんなで　ライジングボルテッカーズ\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "O6XauwBz1Pqv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lyrics.replace(\"\\u3000\", \" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "K6RY_gMA1S_T",
        "outputId": "33c54890-3bf9-4c72-9524-3f91dd5245a1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yo! わたしたちはらいじんぐぼるてっかーず\\nこれからどんなぽけもんにあえるのかな\\nぼうけんがたのしみだ\\n\\n\\nYo! Ready Go!\\nそれならもっとぽけもん まなんでみよう\\nまずはどんがつくぽけもん！\\n3 2 1 Let’s Go!\\n\\n\\nこらいどん たんどん うおちるどん\\nみらいどん どんからす とりとどん\\nつぎはりこといっしょに？\\nAre You Ready?\\n\\n\\nん〜 やれるかな？ いっしょにやってみよう！\\nこらいどん たんどん うおちるどん\\nみらいどん どんからす とりとどん\\n\\n\\nBrave in Heart Heart Heart\\nそらのうえで なみのり\\nどきどき わくわくな\\nぼうけんのはじまり\\nかじをとり せかいへつづくみち\\nなかまのあかしのはんどさいん\\nみんなで らいじんぐぼるてっかーず\\n\\n\\nもっともっとまなんでいこー！\\nつぎはりーじょんふぉーむいるぽけもん！\\nろいにおてほん みせてもらおう！\\n3 2 1 Go!\\n\\n\\nこらった らった がらがら\\nだるまっか びりりだま らっかしたら だいばくはつ\\nつぎはこのひとがやってくれるよ\\nLet’s Go!\\n\\n\\nふりーどだ！おれにまかせとけ！！\\nこらった らった がらがら\\nだるまっか びりりだま らっかしたら だいばくはつ\\n\\n\\nBrave Bird Bird Bird\\nゆうきとともにそらへ\\nFly in the Sky Sky Sky\\nめざすよ あのばしょへ\\nきずつき たまにみち ころんでも\\nなかまのあかしのはんどさいん\\nみんなで らいじんぐぼるてっかーず'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# エンコーディング\n",
        "\n",
        "## テキストを索引（インデックス）のリストに変換すること\n",
        "\n",
        "* ユニークな文字を取得し、それぞれにインデックスを割り当てる\n",
        "\n",
        "* 任意のテキストをエンコーディングする\n",
        "\n",
        "* 吾輩は猫であるの文字種を取得し、それぞれにインデックスを割り当てる\n",
        "\n",
        "* 吾輩は猫であるをエンコーディングする"
      ],
      "metadata": {
        "id": "empWs4GtGgK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = get_yomigana(texts)"
      ],
      "metadata": {
        "id": "PmKQ2Hin_KSz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(txt):\n",
        "  ctoi = {c:i for (i, c) in enumerate(sorted(set(texts)))}\n",
        "  return [ctoi[c] for c in txt]\n",
        "encoded_txt = encode(texts)"
      ],
      "metadata": {
        "id": "xemUqa7z9grB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_txt"
      ],
      "metadata": {
        "id": "r0_UvhLe_bMw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2650f347-a20b-4d0e-fcf6-30ba773794f5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[126,\n",
              " 60,\n",
              " 95,\n",
              " 53,\n",
              " 95,\n",
              " 93,\n",
              " 67,\n",
              " 87,\n",
              " 51,\n",
              " 123,\n",
              " 44,\n",
              " 90,\n",
              " 110,\n",
              " 56,\n",
              " 95,\n",
              " 110,\n",
              " 80,\n",
              " 90,\n",
              " 53,\n",
              " 44,\n",
              " 89,\n",
              " 67,\n",
              " 87,\n",
              " 54,\n",
              " 110,\n",
              " 124,\n",
              " 79,\n",
              " 59,\n",
              " 88,\n",
              " 128,\n",
              " 88,\n",
              " 65,\n",
              " 128,\n",
              " 88,\n",
              " 54,\n",
              " 60,\n",
              " 84,\n",
              " 59,\n",
              " 92,\n",
              " 44,\n",
              " 90,\n",
              " 91,\n",
              " 87,\n",
              " 114,\n",
              " 54,\n",
              " 73,\n",
              " 64,\n",
              " 121,\n",
              " 53,\n",
              " 72,\n",
              " 113,\n",
              " 72,\n",
              " 113,\n",
              " 71,\n",
              " 79,\n",
              " 88,\n",
              " 67,\n",
              " 125,\n",
              " 87,\n",
              " 91,\n",
              " 115,\n",
              " 131,\n",
              " 91,\n",
              " 115,\n",
              " 131,\n",
              " 90,\n",
              " 53,\n",
              " 86,\n",
              " 53,\n",
              " 79,\n",
              " 67,\n",
              " 88,\n",
              " 80,\n",
              " 65,\n",
              " 95,\n",
              " 61,\n",
              " 58,\n",
              " 63,\n",
              " 71,\n",
              " 86,\n",
              " 53,\n",
              " 123,\n",
              " 44,\n",
              " 126,\n",
              " 60,\n",
              " 95,\n",
              " 53,\n",
              " 95,\n",
              " 67,\n",
              " 67,\n",
              " 87,\n",
              " 95,\n",
              " 72,\n",
              " 113,\n",
              " 86,\n",
              " 91,\n",
              " 128,\n",
              " 66,\n",
              " 128,\n",
              " 88,\n",
              " 53,\n",
              " 54,\n",
              " 114,\n",
              " 94,\n",
              " 127,\n",
              " 111,\n",
              " 79,\n",
              " 44,\n",
              " 71,\n",
              " 59,\n",
              " 114,\n",
              " 51,\n",
              " 88,\n",
              " 87,\n",
              " 61,\n",
              " 63,\n",
              " 88,\n",
              " 77,\n",
              " 124,\n",
              " 95,\n",
              " 71,\n",
              " 119,\n",
              " 75,\n",
              " 53,\n",
              " 88,\n",
              " 53,\n",
              " 54,\n",
              " 91,\n",
              " 128,\n",
              " 66,\n",
              " 128,\n",
              " 81,\n",
              " 117,\n",
              " 54,\n",
              " 87,\n",
              " 53,\n",
              " 81,\n",
              " 96,\n",
              " 128,\n",
              " 89,\n",
              " 54,\n",
              " 51,\n",
              " 63,\n",
              " 90,\n",
              " 71,\n",
              " 117,\n",
              " 78,\n",
              " 63,\n",
              " 87,\n",
              " 51,\n",
              " 83,\n",
              " 79,\n",
              " 77,\n",
              " 54,\n",
              " 80,\n",
              " 44,\n",
              " 67,\n",
              " 94,\n",
              " 71,\n",
              " 119,\n",
              " 75,\n",
              " 53,\n",
              " 88,\n",
              " 53,\n",
              " 54,\n",
              " 94,\n",
              " 95,\n",
              " 88,\n",
              " 61,\n",
              " 89,\n",
              " 61,\n",
              " 126,\n",
              " 124,\n",
              " 126,\n",
              " 124,\n",
              " 127,\n",
              " 88,\n",
              " 121,\n",
              " 56,\n",
              " 86,\n",
              " 91,\n",
              " 86,\n",
              " 63,\n",
              " 54,\n",
              " 88,\n",
              " 53,\n",
              " 54,\n",
              " 95,\n",
              " 90,\n",
              " 71,\n",
              " 87,\n",
              " 51,\n",
              " 123,\n",
              " 44,\n",
              " 71,\n",
              " 59,\n",
              " 71,\n",
              " 77,\n",
              " 94,\n",
              " 88,\n",
              " 54,\n",
              " 72,\n",
              " 95,\n",
              " 90,\n",
              " 91,\n",
              " 88,\n",
              " 53,\n",
              " 54,\n",
              " 67,\n",
              " 54,\n",
              " 114,\n",
              " 90,\n",
              " 59,\n",
              " 83,\n",
              " 79,\n",
              " 59,\n",
              " 121,\n",
              " 105,\n",
              " 84,\n",
              " 80,\n",
              " 128,\n",
              " 67,\n",
              " 126,\n",
              " 71,\n",
              " 53,\n",
              " 88,\n",
              " 114,\n",
              " 58,\n",
              " 114,\n",
              " 126,\n",
              " 90,\n",
              " 59,\n",
              " 83,\n",
              " 79,\n",
              " 44,\n",
              " 79,\n",
              " 80,\n",
              " 59,\n",
              " 124,\n",
              " 94,\n",
              " 86,\n",
              " 94,\n",
              " 98,\n",
              " 121,\n",
              " 91,\n",
              " 94,\n",
              " 75,\n",
              " 121,\n",
              " 124,\n",
              " 86,\n",
              " 73,\n",
              " 131,\n",
              " 88,\n",
              " 114,\n",
              " 81,\n",
              " 51,\n",
              " 66,\n",
              " 121,\n",
              " 124,\n",
              " 79,\n",
              " 88,\n",
              " 61,\n",
              " 90,\n",
              " 128,\n",
              " 80,\n",
              " 59,\n",
              " 101,\n",
              " 126,\n",
              " 101,\n",
              " 126,\n",
              " 71,\n",
              " 79,\n",
              " 59,\n",
              " 128,\n",
              " 72,\n",
              " 60,\n",
              " 51,\n",
              " 83,\n",
              " 79,\n",
              " 96,\n",
              " 59,\n",
              " 122,\n",
              " 87,\n",
              " 51,\n",
              " 123,\n",
              " 44,\n",
              " 86,\n",
              " 94,\n",
              " 98,\n",
              " 121,\n",
              " 94,\n",
              " 54,\n",
              " 56,\n",
              " 87,\n",
              " 73,\n",
              " 67,\n",
              " 71,\n",
              " 58,\n",
              " 81,\n",
              " 84,\n",
              " 53,\n",
              " 86,\n",
              " 71,\n",
              " 119,\n",
              " 75,\n",
              " 53,\n",
              " 94,\n",
              " 59,\n",
              " 58,\n",
              " 127,\n",
              " 111,\n",
              " 79,\n",
              " 94,\n",
              " 60,\n",
              " 53,\n",
              " 126,\n",
              " 118,\n",
              " 123,\n",
              " 91,\n",
              " 128,\n",
              " 66,\n",
              " 128,\n",
              " 88,\n",
              " 53,\n",
              " 54,\n",
              " 114,\n",
              " 94,\n",
              " 94,\n",
              " 111,\n",
              " 95,\n",
              " 72,\n",
              " 113,\n",
              " 87,\n",
              " 51,\n",
              " 125,\n",
              " 54,\n",
              " 44,\n",
              " 67,\n",
              " 94,\n",
              " 88,\n",
              " 61,\n",
              " 111,\n",
              " 119,\n",
              " 54,\n",
              " 90,\n",
              " 114,\n",
              " 94,\n",
              " 80,\n",
              " 88,\n",
              " 58,\n",
              " 114,\n",
              " 83,\n",
              " 79,\n",
              " 59,\n",
              " 128,\n",
              " 72,\n",
              " 60,\n",
              " 53,\n",
              " 110,\n",
              " 87,\n",
              " 114,\n",
              " 94,\n",
              " 67,\n",
              " 83,\n",
              " 86,\n",
              " 53,\n",
              " 123,\n",
              " 44,\n",
              " 80,\n",
              " 53,\n",
              " 53,\n",
              " 81,\n",
              " 114,\n",
              " 54,\n",
              " 127,\n",
              " 114,\n",
              " 83,\n",
              " 86,\n",
              " 77,\n",
              " 54,\n",
              " 71,\n",
              " 119,\n",
              " 63,\n",
              " 69,\n",
              " 124,\n",
              " 105,\n",
              " 61,\n",
              " 95,\n",
              " 74,\n",
              " 94,\n",
              " 59,\n",
              " 58,\n",
              " 60,\n",
              " 84,\n",
              " 123,\n",
              " 84,\n",
              " 123,\n",
              " 71,\n",
              " 86,\n",
              " 110,\n",
              " 123,\n",
              " 87,\n",
              " 116,\n",
              " 59,\n",
              " 128,\n",
              " 80,\n",
              " 44,\n",
              " 77,\n",
              " 94,\n",
              " 68,\n",
              " 93,\n",
              " 67,\n",
              " 91,\n",
              " 114,\n",
              " 80,\n",
              " 53,\n",
              " 102,\n",
              " 51,\n",
              " 83,\n",
              " 79,\n",
              " 60,\n",
              " 67,\n",
              " 128,\n",
              " 90,\n",
              " 59,\n",
              " 79,\n",
              " 126,\n",
              " 91,\n",
              " 95,\n",
              " 53,\n",
              " 81,\n",
              " 89,\n",
              " 114,\n",
              " 87,\n",
              " 51,\n",
              " 126,\n",
              " 71,\n",
              " 79,\n",
              " 67,\n",
              " 88,\n",
              " 60,\n",
              " 90,\n",
              " 53,\n",
              " 44,\n",
              " 94,\n",
              " 111,\n",
              " 90,\n",
              " 121,\n",
              " 74,\n",
              " 59,\n",
              " 58,\n",
              " 94,\n",
              " 110,\n",
              " 128,\n",
              " 90,\n",
              " 59,\n",
              " 60,\n",
              " 51,\n",
              " 110,\n",
              " 122,\n",
              " 91,\n",
              " 88,\n",
              " 83,\n",
              " 61,\n",
              " 71,\n",
              " 86,\n",
              " 53,\n",
              " 123,\n",
              " 44,\n",
              " 77,\n",
              " 54,\n",
              " 71,\n",
              " 86,\n",
              " 77,\n",
              " 94,\n",
              " 51,\n",
              " 90,\n",
              " 94,\n",
              " 90,\n",
              " 59,\n",
              " 59,\n",
              " 121,\n",
              " 88,\n",
              " 61,\n",
              " 89,\n",
              " 61,\n",
              " 103,\n",
              " 54,\n",
              " 103,\n",
              " 54,\n",
              " 88,\n",
              " 65,\n",
              " 112,\n",
              " 122,\n",
              " 127,\n",
              " 101,\n",
              " 63,\n",
              " 44,\n",
              " 89,\n",
              " 54,\n",
              " 114,\n",
              " 94,\n",
              " 128,\n",
              " 89,\n",
              " 75,\n",
              " 109,\n",
              " 63,\n",
              " 86,\n",
              " 72,\n",
              " 84,\n",
              " 91,\n",
              " 120,\n",
              " 126,\n",
              " 83,\n",
              " 79,\n",
              " 44,\n",
              " 67,\n",
              " 124,\n",
              " 60,\n",
              " 91,\n",
              " 128,\n",
              " 66,\n",
              " 128,\n",
              " 94,\n",
              " 94,\n",
              " 112,\n",
              " 79,\n",
              " 96,\n",
              " 67,\n",
              " 88,\n",
              " 53,\n",
              " 54,\n",
              " 114,\n",
              " 94,\n",
              " 87,\n",
              " 51,\n",
              " 123,\n",
              " 67,\n",
              " 88,\n",
              " 95,\n",
              " 120,\n",
              " 54,\n",
              " 116,\n",
              " 63,\n",
              " 67,\n",
              " 94,\n",
              " 67,\n",
              " 125,\n",
              " 71,\n",
              " 83,\n",
              " 79,\n",
              " 44,\n",
              " 67,\n",
              " 94,\n",
              " 71,\n",
              " 119,\n",
              " 75,\n",
              " 53,\n",
              " 94,\n",
              " 86,\n",
              " 94,\n",
              " 98,\n",
              " 121,\n",
              " 94,\n",
              " 54,\n",
              " 121,\n",
              " 87,\n",
              " 71,\n",
              " 96,\n",
              " 121,\n",
              " 63,\n",
              " 95,\n",
              " 120,\n",
              " 53,\n",
              " 67,\n",
              " 67,\n",
              " 125,\n",
              " 114,\n",
              " 81,\n",
              " 91,\n",
              " 73,\n",
              " 126,\n",
              " 83,\n",
              " 86,\n",
              " 58,\n",
              " 83,\n",
              " 79,\n",
              " 60,\n",
              " 43,\n",
              " 71,\n",
              " 96,\n",
              " 121,\n",
              " 63,\n",
              " 73,\n",
              " 123,\n",
              " 88,\n",
              " 98,\n",
              " 72,\n",
              " 119,\n",
              " 54,\n",
              " 90,\n",
              " 77,\n",
              " 63,\n",
              " 122,\n",
              " 119,\n",
              " 63,\n",
              " 87,\n",
              " 54,\n",
              " 128,\n",
              " 86,\n",
              " 128,\n",
              " 71,\n",
              " 95,\n",
              " 72,\n",
              " 113,\n",
              " 79,\n",
              " 44,\n",
              " 71,\n",
              " 119,\n",
              " 75,\n",
              " 53,\n",
              " 60,\n",
              " 54,\n",
              " 68,\n",
              " 63,\n",
              " 94,\n",
              " 59,\n",
              " 72,\n",
              " 102,\n",
              " 128,\n",
              " 80,\n",
              " 65,\n",
              " 60,\n",
              " 54,\n",
              " 68,\n",
              " 63,\n",
              " 94,\n",
              " 59,\n",
              " 126,\n",
              " 59,\n",
              " 121,\n",
              " 90,\n",
              " 53,\n",
              " 60,\n",
              " 112,\n",
              " 51,\n",
              " 128,\n",
              " 91,\n",
              " 113,\n",
              " 60,\n",
              " 113,\n",
              " 64,\n",
              " 123,\n",
              " 44,\n",
              " 112,\n",
              " 93,\n",
              " 60,\n",
              " 126,\n",
              " 123,\n",
              " 63,\n",
              " 90,\n",
              " 123,\n",
              " 44,\n",
              " 88,\n",
              " 54,\n",
              " 86,\n",
              " 53,\n",
              " 79,\n",
              " 73,\n",
              " 59,\n",
              " 121,\n",
              " 90,\n",
              " 53,\n",
              " 88,\n",
              " 58,\n",
              " 114,\n",
              " 83,\n",
              " 86,\n",
              " 53,\n",
              " 123,\n",
              " 88,\n",
              " 43,\n",
              " 89,\n",
              " 69,\n",
              " 122,\n",
              " 88,\n",
              " 58,\n",
              " 88,\n",
              " 60,\n",
              " 71,\n",
              " 86,\n",
              " 113,\n",
              " 59,\n",
              " 121,\n",
              " 98,\n",
              " 60,\n",
              " 87,\n",
              " 79,\n",
              " 44,\n",
              " 77,\n",
              " 124,\n",
              " 110,\n",
              " 87,\n",
              " 95,\n",
              " 61,\n",
              " 58,\n",
              " 63,\n",
              " 71,\n",
              " 86,\n",
              " 53,\n",
              " 123,\n",
              " 60,\n",
              " 51,\n",
              " 88,\n",
              " 95,\n",
              " 90,\n",
              " 91,\n",
              " 94,\n",
              " 67,\n",
              " 88,\n",
              " 116,\n",
              " 121,\n",
              " 53,\n",
              " 63,\n",
              " 121,\n",
              " 59,\n",
              " 128,\n",
              " 60,\n",
              " 56,\n",
              " 80,\n",
              " 77,\n",
              " 54,\n",
              " 88,\n",
              " 71,\n",
              " 86,\n",
              " 114,\n",
              " 126,\n",
              " 59,\n",
              " 121,\n",
              " 90,\n",
              " 53,\n",
              " 44,\n",
              " 101,\n",
              " 88,\n",
              " 61,\n",
              " 60,\n",
              " 84,\n",
              " 53,\n",
              " 86,\n",
              " 111,\n",
              " 123,\n",
              " 88,\n",
              " 71,\n",
              " 119,\n",
              " 75,\n",
              " 53,\n",
              " 95,\n",
              " 53,\n",
              " 90,\n",
              " 53,\n",
              " 44,\n",
              " 79,\n",
              " 63,\n",
              " 69,\n",
              " 128,\n",
              " 58,\n",
              " 83,\n",
              " 79,\n",
              " 61,\n",
              " 119,\n",
              " 54,\n",
              " 80,\n",
              " 53,\n",
              " 60,\n",
              " 53,\n",
              " 83,\n",
              " 100,\n",
              " 61,\n",
              " 114,\n",
              " 111,\n",
              " 56,\n",
              " 92,\n",
              " 44,\n",
              " 59,\n",
              " 128,\n",
              " 72,\n",
              " 128,\n",
              " 94,\n",
              " 95,\n",
              " 95,\n",
              " 58,\n",
              " 116,\n",
              " 69,\n",
              " 56,\n",
              " 73,\n",
              " 60,\n",
              " 79,\n",
              " 127,\n",
              " 59,\n",
              " 63,\n",
              " 71,\n",
              " 86,\n",
              " 71,\n",
              " 110,\n",
              " 83,\n",
              " 79,\n",
              " 44,\n",
              " 77,\n",
              " 94,\n",
              " 59,\n",
              " 111,\n",
              " 53,\n",
              " 110,\n",
              " 110,\n",
              " 87,\n",
              " 94,\n",
              " 88,\n",
              " 67,\n",
              " 125,\n",
              " 88,\n",
              " 95,\n",
              " 81,\n",
              " 60,\n",
              " 83,\n",
              " 86,\n",
              " 112,\n",
              " 51,\n",
              " 128,\n",
              " 91,\n",
              " 51,\n",
              " 59,\n",
              " 123,\n",
              " 53,\n",
              " 44,\n",
              " 113,\n",
              " 127,\n",
              " 51,\n",
              " 53,\n",
              " 86,\n",
              " 53,\n",
              " 121,\n",
              " 124,\n",
              " 92,\n",
              " 63,\n",
              " 121,\n",
              " 53,\n",
              " 80,\n",
              " 44,\n",
              " 95,\n",
              " 86,\n",
              " 90,\n",
              " 90,\n",
              " 91,\n",
              " 87,\n",
              " 114,\n",
              " 120,\n",
              " 54,\n",
              " 73,\n",
              " 60,\n",
              " 58,\n",
              " 59,\n",
              " 71,\n",
              " 53,\n",
              " 88,\n",
              " 43,\n",
              " 94,\n",
              " 77,\n",
              " 94,\n",
              " 77,\n",
              " 95,\n",
              " 53,\n",
              " 80,\n",
              " 71,\n",
              " 86,\n",
              " 111,\n",
              " 123,\n",
              " 88,\n",
              " 98,\n",
              " 72,\n",
              " 119,\n",
              " 54,\n",
              " 91,\n",
              " 53,\n",
              " 79,\n",
              " 53,\n",
              " 44,\n",
              " 126,\n",
              " 60,\n",
              " 95,\n",
              " 53,\n",
              " 95,\n",
              " 126,\n",
              " 121,\n",
              " 94,\n",
              " 54,\n",
              " 56,\n",
              " 59,\n",
              " 121,\n",
              " 61,\n",
              " 117,\n",
              " 54,\n",
              " 91,\n",
              " 69,\n",
              " 69,\n",
              " 95,\n",
              " 121,\n",
              " 94,\n",
              " 90,\n",
              " 59,\n",
              " 104,\n",
              " 73,\n",
              " 86,\n",
              " 121,\n",
              " 124,\n",
              " 79,\n",
              " 94,\n",
              " 87,\n",
              " 51,\n",
              " 123,\n",
              " 44,\n",
              " 120,\n",
              " 54,\n",
              " 116,\n",
              " 63,\n",
              " 94,\n",
              " 58,\n",
              " 114,\n",
              " 53,\n",
              " 87,\n",
              " 69,\n",
              " 69,\n",
              " 95,\n",
              " 121,\n",
              " 127,\n",
              " 95,\n",
              " 53,\n",
              " 80,\n",
              " 73,\n",
              " 88,\n",
              " 112,\n",
              " 67,\n",
              " 54,\n",
              " 91,\n",
              " 58,\n",
              " 58,\n",
              " 61,\n",
              " 90,\n",
              " 53,\n",
              " 65,\n",
              " 60,\n",
              " 51,\n",
              " 123,\n",
              " 44,\n",
              " 126,\n",
              " 60,\n",
              " 95,\n",
              " 53,\n",
              " 95,\n",
              " 53,\n",
              " 65,\n",
              " 94,\n",
              " 110,\n",
              " 56,\n",
              " 91,\n",
              " 73,\n",
              " 126,\n",
              " 83,\n",
              " 86,\n",
              " 89,\n",
              " 54,\n",
              " 71,\n",
              " 79,\n",
              " 121,\n",
              " 120,\n",
              " 59,\n",
              " 125,\n",
              " 54,\n",
              " 88,\n",
              " 59,\n",
              " 128,\n",
              " 60,\n",
              " 56,\n",
              " 86,\n",
              " 111,\n",
              " 79,\n",
              " 44,\n",
              " 105,\n",
              " 84,\n",
              " 91,\n",
              " 67,\n",
              " 124,\n",
              " 88,\n",
              " 53,\n",
              " 54,\n",
              " 101,\n",
              " 128,\n",
              " 105,\n",
              " 84,\n",
              " 114,\n",
              " 87,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# デコーディング\n",
        "## 索引(インデックス)から単語の列（もしくは文字列）に変換すること"
      ],
      "metadata": {
        "id": "d5-ic9UuuhXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(idx):\n",
        "  itoc = {i:c for (i, c) in enumerate(sorted(set(texts)))}\n",
        "  return ''.join([itoc[i] for i in idx])\n",
        "\n",
        "decoded = decode(encoded_txt[:10])"
      ],
      "metadata": {
        "id": "K1vSnP0g_1oh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(\"lambda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNyB0qva3ASY",
        "outputId": "35d486b4-b70f-4fac-c64f-9a034199d6f9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lambdas\n",
            "*******\n",
            "\n",
            "   lambda_expr ::= \"lambda\" [parameter_list] \":\" expression\n",
            "\n",
            "Lambda expressions (sometimes called lambda forms) are used to create\n",
            "anonymous functions. The expression \"lambda parameters: expression\"\n",
            "yields a function object.  The unnamed object behaves like a function\n",
            "object defined with:\n",
            "\n",
            "   def <lambda>(parameters):\n",
            "       return expression\n",
            "\n",
            "See section Function definitions for the syntax of parameter lists.\n",
            "Note that functions created with lambda expressions cannot contain\n",
            "statements or annotations.\n",
            "\n",
            "Related help topics: FUNCTIONS\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(\"lambda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2js4cyG3olW",
        "outputId": "96b3b459-f030-40a5-93a4-5c67f0ea3821"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lambdas\n",
            "*******\n",
            "\n",
            "   lambda_expr ::= \"lambda\" [parameter_list] \":\" expression\n",
            "\n",
            "Lambda expressions (sometimes called lambda forms) are used to create\n",
            "anonymous functions. The expression \"lambda parameters: expression\"\n",
            "yields a function object.  The unnamed object behaves like a function\n",
            "object defined with:\n",
            "\n",
            "   def <lambda>(parameters):\n",
            "       return expression\n",
            "\n",
            "See section Function definitions for the syntax of parameter lists.\n",
            "Note that functions created with lambda expressions cannot contain\n",
            "statements or annotations.\n",
            "\n",
            "Related help topics: FUNCTIONS\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ctoi = {c:i for (i, c) in enumerate(sorted(set(texts)))}\n",
        "itoc = {i:c for (i, c) in enumerate(sorted(set(texts)))}"
      ],
      "metadata": {
        "id": "vOcAyYxKBA28"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode = lambda s:[ctoi[c] for c in s]\n",
        "decode = lambda l:''.join([itoc[i] for i in l])"
      ],
      "metadata": {
        "id": "YT3Wdlp0LRtr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decode(encode(texts[:10]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "U34jsBZRLqHR",
        "outputId": "b7e854c1-a8b9-43e1-efa9-07a112d5dc93"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'わがはいはねこである'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 訓練データと検証データの作成（Pytorchのtensor型に変換してsplit）"
      ],
      "metadata": {
        "id": "ojqS8X7uMB2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "data = torch.tensor(encode(texts), dtype=torch.long)"
      ],
      "metadata": {
        "id": "00pcFXOYLubG"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bszDcQqRnrN",
        "outputId": "7c152ca0-75c2-4e7a-9953-0d3ec9581acc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([395470])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMATuTgmSCQn",
        "outputId": "b516a7ff-36e0-4d6f-a380-e26867b27fc2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([126,  60,  95,  53,  95,  93,  67,  87,  51, 123,  44,  90, 110,  56,\n",
              "         95, 110,  80,  90,  53,  44,  89,  67,  87,  54, 110, 124,  79,  59,\n",
              "         88, 128,  88,  65, 128,  88,  54,  60,  84,  59,  92,  44,  90,  91,\n",
              "         87, 114,  54,  73,  64, 121,  53,  72])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "test_data = data[n:]"
      ],
      "metadata": {
        "id": "1MbC46mcSDRn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data), len(test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNPGcRmGSV_Q",
        "outputId": "c1e5e092-91e0-478d-9c2f-169c6ddbb2c0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "355923 39547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 大規模言語モデル（LLM: Large Language Models）の本質\n",
        "\n",
        "・次のトークン予測（今回は文字予測）タスク用データの準備\n",
        "\n",
        "・GPUを使った訓練"
      ],
      "metadata": {
        "id": "KFBOOohxUZjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDooktJgfD_c",
        "outputId": "4b50d025-7087-4848-ef00-a0333fdb15fb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([126,  60,  95,  53,  95,  93,  67,  87,  51])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size+1]\n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t].item() #pytorch tutorial\n",
        "  print(f\"入力：{context}, 出力:{target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0DGk7eQgi76",
        "outputId": "9b5d2e02-b7b2-4229-b5dc-0f97760eb686"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "入力：tensor([126]), 出力:60\n",
            "入力：tensor([126,  60]), 出力:95\n",
            "入力：tensor([126,  60,  95]), 出力:53\n",
            "入力：tensor([126,  60,  95,  53]), 出力:95\n",
            "入力：tensor([126,  60,  95,  53,  95]), 出力:93\n",
            "入力：tensor([126,  60,  95,  53,  95,  93]), 出力:67\n",
            "入力：tensor([126,  60,  95,  53,  95,  93,  67]), 出力:87\n",
            "入力：tensor([126,  60,  95,  53,  95,  93,  67,  87]), 出力:51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "HfBzTvXtjKZ4"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "batch_size = 4\n",
        "torch.manual_seed(825)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YijKAQehjiCe",
        "outputId": "a436e7e4-39da-49ec-f6da-8a27b8df4ac6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f70265232f0>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.randint(len(data) - block_size, (batch_size, ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwfoaAaxkfwd",
        "outputId": "ddeffe2a-a81b-4090-cf05-fd5584067ea7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([105183,  23016, 179976, 116576])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.stack([torch.tensor([1,2,3]), torch.tensor([4,5,6])])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH1fH_EukmT5",
        "outputId": "d11a5da9-f96d-4cbc-aa98-ad5dfba8b4d4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3],\n",
              "        [4, 5, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "  \"\"\" ミニバッチを取得\n",
        "    split(str): train or test\n",
        "  \"\"\"\n",
        "  data = train_data if split == 'train' else test_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x.to(device), y.to(device)"
      ],
      "metadata": {
        "id": "AKJQ3TlPjmRE"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = get_batch('train')"
      ],
      "metadata": {
        "id": "UfShApD0lIim"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uSO-W8rlLRE",
        "outputId": "3c37645e-9411-457b-ead1-1cde21e21337"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "474ZL5cFlL6B",
        "outputId": "99582975-88bf-491f-def8-3a5b7602b190"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evDKm7qNlM4d",
        "outputId": "ab81d4ca-abd6-4ce6-b184-b618b14c871e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 71,  61,  90, 121, 121, 128, 110,  88],\n",
              "        [128,  94,  59, 128,  59,  63, 127,  88],\n",
              "        [114, 108,  63, 127,  71, 119,  54,  61],\n",
              "        [131,  88, 114,  81,  51,  66, 121, 124]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yb"
      ],
      "metadata": {
        "id": "9jcFgSZ9lQr0",
        "outputId": "86dca3c5-4003-4806-f7f7-00b5b1996aad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 61,  90, 121, 121, 128, 110,  88,  53],\n",
              "        [ 94,  59, 128,  59,  63, 127,  88,  83],\n",
              "        [108,  63, 127,  71, 119,  54,  61, 119],\n",
              "        [ 88, 114,  81,  51,  66, 121, 124,  79]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 最も単純な2-gramモデルを訓練\n",
        "\n",
        "* nn.Moduleクラスを継承"
      ],
      "metadata": {
        "id": "YRU6wB1LOQBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(set(texts))\n",
        "vocab_size"
      ],
      "metadata": {
        "id": "cietzlVxlU60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f95fe7fe-4dd8-4135-b358-29a6f7c3799c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "673"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(set([i.item() for i in data]))\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgZUiqdlPrZO",
        "outputId": "422a870f-9fde-46dc-b761-de67b7515c76"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "673"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vmypRfuPJgR",
        "outputId": "5267b291-1596-4147-f3cc-a46d7562c40c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([126,  60,  95,  53,  95,  93,  67,  87,  51, 123])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[:10]"
      ],
      "metadata": {
        "id": "S92t2rnDQKgX",
        "outputId": "dd6a7d10-cd0c-4ea5-9f1e-0d9b55fbbf20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'わがはいはねこである'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "Ukw6SjtEQLsn"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size).to(device)\n",
        "\n",
        "  def forward(self, idx, targets):\n",
        "    logits = self.token_embedding_table(idx)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "1YmgMnEppxvh"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = BigramLanguageModel(vocab_size)"
      ],
      "metadata": {
        "id": "Ls7rYTe9sCtH"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = m(xb, yb)"
      ],
      "metadata": {
        "id": "By4tk0wHsGJH"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9UcysZgsuHU",
        "outputId": "4461cdc8-0099-4670-c53c-4fe518c67deb"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoM0XA8vtLgX",
        "outputId": "26d8d9c9-694f-4456-9268-eea327cf9f6d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 673])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nu4SMR81sXDY",
        "outputId": "c9c42f69-7d2f-441d-bb1d-4a37b616d0d0"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.0238,  0.4836, -1.2794,  ..., -0.8560,  0.6900, -1.2224],\n",
              "         [ 0.0098, -0.4977, -1.6767,  ...,  0.8834,  1.0018,  1.7431],\n",
              "         [ 1.9313,  1.3786, -1.0101,  ...,  0.3446,  1.2663,  0.2734],\n",
              "         ...,\n",
              "         [-1.5288, -0.5065, -1.2861,  ..., -1.0272, -0.7021,  1.1295],\n",
              "         [-0.5599,  0.2612, -1.7893,  ..., -0.3391,  0.0912,  0.4208],\n",
              "         [ 1.9650, -0.8913,  0.5921,  ...,  0.3632,  0.2208, -0.8550]],\n",
              "\n",
              "        [[-1.5288, -0.5065, -1.2861,  ..., -1.0272, -0.7021,  1.1295],\n",
              "         [ 1.2050,  0.7333,  0.0316,  ..., -0.5102,  0.2448, -0.8419],\n",
              "         [ 0.8267,  0.2788, -2.6456,  ..., -1.5758, -1.2271,  0.5149],\n",
              "         ...,\n",
              "         [ 0.5166, -0.0446,  0.9283,  ..., -1.3275,  1.5114,  0.8338],\n",
              "         [-0.0405, -0.6578,  0.7340,  ...,  1.2118, -0.3390, -1.0983],\n",
              "         [ 1.9650, -0.8913,  0.5921,  ...,  0.3632,  0.2208, -0.8550]],\n",
              "\n",
              "        [[ 0.5672,  0.4672, -0.5015,  ...,  1.0559,  0.2809,  1.2613],\n",
              "         [-1.5352,  0.7513, -1.1041,  ...,  0.1750,  1.2243,  1.8182],\n",
              "         [ 0.5166, -0.0446,  0.9283,  ..., -1.3275,  1.5114,  0.8338],\n",
              "         ...,\n",
              "         [ 0.8955,  1.0258,  0.8024,  ..., -0.4821,  0.0963, -0.8880],\n",
              "         [ 0.4206,  0.3900, -0.6126,  ...,  0.1046,  1.5044,  1.3615],\n",
              "         [ 0.0098, -0.4977, -1.6767,  ...,  0.8834,  1.0018,  1.7431]],\n",
              "\n",
              "        [[-0.7545,  0.6255,  0.6126,  ..., -0.8699, -0.0370, -0.1951],\n",
              "         [ 1.9650, -0.8913,  0.5921,  ...,  0.3632,  0.2208, -0.8550],\n",
              "         [ 0.5672,  0.4672, -0.5015,  ...,  1.0559,  0.2809,  1.2613],\n",
              "         ...,\n",
              "         [-0.2498,  0.2651,  0.0832,  ..., -0.2178, -1.6653,  0.2897],\n",
              "         [-2.0493,  1.9546,  0.5341,  ..., -1.2952,  2.7026, -0.3624],\n",
              "         [ 1.1572, -0.2695,  0.6918,  ...,  1.1985,  0.3984, -0.5372]]],\n",
              "       device='cuda:0', grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size).to(device)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    logits = self.token_embedding_table(idx)\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(-1, C)\n",
        "      targets = targets.view(-1)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_tokens):\n",
        "    for _ in range(max_tokens):\n",
        "      # 予測\n",
        "      logits, loss = self(idx)\n",
        "      # 最後のタイムステップ（文字）にフォーカス\n",
        "      logits = logits[:, -1, :] # (B, C)\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "BFcmIFd5srTe"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel(vocab_size)"
      ],
      "metadata": {
        "id": "wU4hJUouvRQ-"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = model.to(device)"
      ],
      "metadata": {
        "id": "gQmRXXFG0Esq"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits, loss = m(xb, yb)"
      ],
      "metadata": {
        "id": "W7D0ekhp0H-J"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(logits.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-SLybEZ0J1M",
        "outputId": "39e6cfc2-3e52-4483-84c7-c5b5d58727e3"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 673])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftiDFeTk0N2C",
        "outputId": "69edd4a4-3221-459a-f890-e8ef201b0b52"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(7.2310, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1, 1), dtype=torch.long).to(device)"
      ],
      "metadata": {
        "id": "yVt15ftu0QL6"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNIAMoN_0YGy",
        "outputId": "58d69d8c-3ff7-4ebb-cb62-ec5830864131"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(sorted(set(texts)))[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "G8CY3K_D0YZ8",
        "outputId": "b1ea7023-be1c-467c-ab1f-e8b9392913cc"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode(m.generate(idx, max_tokens=10)[0].tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5aj0AjY20jbY",
        "outputId": "7815f6c9-50f7-49c9-f160-17d9fc3a24cc"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' 談淋に這d箝客愧事政'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 訓練コード"
      ],
      "metadata": {
        "id": "6heyoyUJf6VW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters = 20000\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "yrDbHiPd0qQZ"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=32"
      ],
      "metadata": {
        "id": "-xLXQwkggdgw"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_iters = 200\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  m.eval()\n",
        "  for split in ['train', 'test']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = m(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  m.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "FatYEMEqpnbS"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_interval = 300\n",
        "for steps in range(max_iters):\n",
        "  if steps % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step: {steps}, train_loss: {losses['train']:.4f}, test loss {losses['test']:.4f}\")\n",
        "  xb, yb = get_batch(\"train\")\n",
        "  logits, loss = m(xb, yb) #推論\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if steps % 1000 == 0:\n",
        "    print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inGR00ftgf_i",
        "outputId": "04f329dd-0a46-4d6b-a5b1-d2fb89e187ff"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0, train_loss: 7.0100, test loss 6.9989\n",
            "tensor(6.8862, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "step: 300, train_loss: 6.6630, test loss 6.6591\n",
            "step: 600, train_loss: 6.3391, test loss 6.3233\n",
            "step: 900, train_loss: 6.0144, test loss 6.0115\n",
            "tensor(5.8402, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "step: 1200, train_loss: 5.7190, test loss 5.7221\n",
            "step: 1500, train_loss: 5.4440, test loss 5.4560\n",
            "step: 1800, train_loss: 5.2065, test loss 5.2191\n",
            "tensor(5.0970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "step: 2100, train_loss: 4.9757, test loss 5.0008\n",
            "step: 2400, train_loss: 4.7806, test loss 4.8013\n",
            "step: 2700, train_loss: 4.6062, test loss 4.6253\n",
            "step: 3000, train_loss: 4.4359, test loss 4.4631\n",
            "tensor(4.3844, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "step: 3300, train_loss: 4.3060, test loss 4.3313\n",
            "step: 3600, train_loss: 4.1808, test loss 4.2062\n",
            "step: 3900, train_loss: 4.0700, test loss 4.1007\n",
            "tensor(4.1491, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "step: 4200, train_loss: 3.9889, test loss 4.0154\n",
            "step: 4500, train_loss: 3.8966, test loss 3.9437\n",
            "step: 4800, train_loss: 3.8254, test loss 3.8788\n",
            "tensor(3.8606, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "step: 5100, train_loss: 3.7676, test loss 3.8099\n",
            "step: 5400, train_loss: 3.7169, test loss 3.7648\n",
            "step: 5700, train_loss: 3.6768, test loss 3.7120\n",
            "step: 6000, train_loss: 3.6181, test loss 3.6809\n",
            "tensor(3.6429, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "step: 6300, train_loss: 3.6005, test loss 3.6492\n",
            "step: 6600, train_loss: 3.5690, test loss 3.6264\n",
            "step: 6900, train_loss: 3.5457, test loss 3.5948\n",
            "tensor(3.4354, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "step: 7200, train_loss: 3.5297, test loss 3.5840\n",
            "step: 7500, train_loss: 3.4957, test loss 3.5481\n",
            "step: 7800, train_loss: 3.4803, test loss 3.5350\n",
            "tensor(3.4893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "step: 8100, train_loss: 3.4811, test loss 3.5079\n",
            "step: 8400, train_loss: 3.4567, test loss 3.5096\n",
            "step: 8700, train_loss: 3.4407, test loss 3.4911\n",
            "step: 9000, train_loss: 3.4388, test loss 3.4808\n",
            "tensor(3.5042, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "step: 9300, train_loss: 3.4300, test loss 3.4889\n",
            "step: 9600, train_loss: 3.4175, test loss 3.4796\n",
            "step: 9900, train_loss: 3.4091, test loss 3.4612\n",
            "tensor(3.4515, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "step: 10200, train_loss: 3.4107, test loss 3.4657\n",
            "step: 10500, train_loss: 3.3904, test loss 3.4488\n",
            "step: 10800, train_loss: 3.3951, test loss 3.4520\n",
            "tensor(3.4233, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "step: 11100, train_loss: 3.3959, test loss 3.4544\n",
            "step: 11400, train_loss: 3.3943, test loss 3.4526\n",
            "step: 11700, train_loss: 3.3847, test loss 3.4485\n",
            "step: 12000, train_loss: 3.3782, test loss 3.4399\n",
            "tensor(3.2936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "step: 12300, train_loss: 3.3846, test loss 3.4382\n",
            "step: 12600, train_loss: 3.3822, test loss 3.4418\n",
            "step: 12900, train_loss: 3.3759, test loss 3.4336\n",
            "tensor(3.4216, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "step: 13200, train_loss: 3.3689, test loss 3.4338\n",
            "step: 13500, train_loss: 3.3651, test loss 3.4244\n",
            "step: 13800, train_loss: 3.3600, test loss 3.4255\n",
            "tensor(3.3892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "step: 14100, train_loss: 3.3663, test loss 3.4381\n",
            "step: 14400, train_loss: 3.3656, test loss 3.4217\n",
            "step: 14700, train_loss: 3.3612, test loss 3.4224\n",
            "step: 15000, train_loss: 3.3669, test loss 3.4333\n",
            "tensor(3.3481, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "step: 15300, train_loss: 3.3664, test loss 3.4188\n",
            "step: 15600, train_loss: 3.3596, test loss 3.4256\n",
            "step: 15900, train_loss: 3.3580, test loss 3.4204\n",
            "tensor(3.2611, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "step: 16200, train_loss: 3.3614, test loss 3.4088\n",
            "step: 16500, train_loss: 3.3537, test loss 3.4255\n",
            "step: 16800, train_loss: 3.3555, test loss 3.4039\n",
            "tensor(3.3867, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "step: 17100, train_loss: 3.3518, test loss 3.4272\n",
            "step: 17400, train_loss: 3.3508, test loss 3.4176\n",
            "step: 17700, train_loss: 3.3617, test loss 3.4239\n",
            "step: 18000, train_loss: 3.3558, test loss 3.4104\n",
            "tensor(3.3839, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "step: 18300, train_loss: 3.3479, test loss 3.4179\n",
            "step: 18600, train_loss: 3.3607, test loss 3.4221\n",
            "step: 18900, train_loss: 3.3449, test loss 3.4126\n",
            "tensor(3.3851, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "step: 19200, train_loss: 3.3546, test loss 3.4151\n",
            "step: 19500, train_loss: 3.3503, test loss 3.4146\n",
            "step: 19800, train_loss: 3.3478, test loss 3.4223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode(m.generate(idx, max_tokens=100)[0].tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8wsReHiTg6-R",
        "outputId": "88afeb95-047b-4f0e-82a9-9cde6b9d15e3"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' 易倆事m痞忽度学きたくにからいたずかりもうぶんをとこんざしいろうしない。かにてきっていでこにしへでにんとから、て、けんかげんじまれれむかもおものでするべかえてもって、ひろいた。それだけのおうにがいるた'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[9000:10000]"
      ],
      "metadata": {
        "id": "Yv1waazthCLA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "outputId": "2d0a2bca-497e-4752-ed18-cce0ed619fb7"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'はらのたしになるもんか」かれはだいにかんしゃくにさわったようすで、かんちくをそいだようなみみをしきりとぴくつかせてあららかにたちさった。わがはいがくるまやのくろとちきになったのはこれからである。そのごわがはいはたびたびくろとかいこうする。かいこうするごとにかれはくるまやそうとうのきえんをはく。さきにわがはいがみみにしたというふとくじけんもじつはくろからきいたのである。あるるひれいのごとくわがはいとくろはあたたかいちゃはたけのなかでねころびながらいろいろざつだんをしていると、かれはいつものじまんはなしをさもあたらしそうにくりかえしたあとで、わがはいにむかってしたのごとくしつもんした。「ごめえはいままでにねずみをなんひきとったことがある」ちしきはくろよりもよほどはったつしているつもりだがわんりょくとゆうきとにいたってはとうていくろのひかくにはならないとかくごはしていたものの、このといにせっしたるときは、さすがにきまりがよくはなかった。けれどもじじつはじじつで詐るわけにはいかないから、わがはいは「じつはとろうとろうとおもってまだとらない」とこたえた。くろはかれのはなのさきからぴんと突張っているながいひげをびりびりとふるわせてひじょうにわらった。がんらいくろはじまんをするたけにどこかたりないところがあって、かれのきえんをかんしんしたようにいんこうをころころならしてきんちょうしていればはなはだぎょしやすいねこである。わがはいはかれとこんづけになってからじかにこのこきゅうをのみこんだからこのばあいにもなまじいおのれれをべんごしてますますけいせいをわるくするのもぐである、いっそのことかれにじぶんのてがらばなしをしゃべらしてごちゃをにごすにわかくはないとしあんをさだめた。そこでおとなしく「きみなどはとしがとしであるからだいぶとったろう」とそそのかしてみた。かぜんかれは墻壁のけつしょにとっかんしてきた。「たんとでもねえがさんよんじゅうはとったろう」とはとくいげなるかれのこたえであった。かれはなおかたりをつづけて「ねずみのひゃくやにひゃくはいちにんでいつでもひきうけるがいたちってえやつはてにあわねえ。いちどいたちにむかってひどいめにあった」「へえなるほど」とあいづちをうつ。くろはおおきなめをぱちつかせていう。「きょねんのだいそうじのときだ。うちのていしゅがせっかいのふくろをもって椽のし'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-Attentionの考え方\n",
        "0. 言語モデルで現在のトークン（過去のトークンの平均）をfor文で単純に得る方法\n",
        "1. 言語モデルで現在のトークンが持っている情報を効率的に得る方法 (attentionではない場合（平均））\n",
        "2. Attention(Key, Query, Value)機構を効率的に得る方法"
      ],
      "metadata": {
        "id": "34auqg1YTLLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.manual_seed(826)\n",
        "\n",
        "B, T, C = 4, 8, 2\n",
        "x = torch.randn(B, T, C)\n",
        "x.shape"
      ],
      "metadata": {
        "id": "ZMOPZlm6hbdC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b61a19f7-5650-4e19-9603-eb8938996b6f"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "わ→が→は→い→は→ね→こ→で→あ→る"
      ],
      "metadata": {
        "id": "sBGcE6a7fT28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xbow = torch.zeros((B, T, C))\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b, :t+1] # (t, C)\n",
        "    xbow[b, t] = torch.mean(xprev, 0)\n",
        "xbow[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCeWSOs7TS4P",
        "outputId": "7867b11e-b23c-4ea7-a22f-1788cbd149a3"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.3604, -1.5006],\n",
              "        [ 0.7338, -0.6960],\n",
              "        [ 0.5539, -0.1388],\n",
              "        [ 0.3351,  0.2609],\n",
              "        [ 0.1492,  0.3077],\n",
              "        [ 0.0892,  0.1931],\n",
              "        [-0.0351,  0.1505],\n",
              "        [ 0.0029,  0.3566]])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "o = torch.ones(3,3)\n",
        "a = torch.tril(o)"
      ],
      "metadata": {
        "id": "QyqpGZVXmZy8"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = a / a.sum(1, keepdim=True)"
      ],
      "metadata": {
        "id": "4p5FPeBpm7qJ"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctFvRkp7naBV",
        "outputId": "d5e08ceb-d16f-4f0c-d041-8a60b408ec91"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333]])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = torch.randint(0, 10, (3,2)).float()\n",
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaQnxnPGmfn5",
        "outputId": "0619459b-1edc-4111-a4f9-3a148c4ca4da"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[8., 5.],\n",
              "        [5., 0.],\n",
              "        [7., 3.]])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a@b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18nfq9WJmwmU",
        "outputId": "b2234768-572b-4fb5-e67e-267548e1e816"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[8.0000, 5.0000],\n",
              "        [6.5000, 2.5000],\n",
              "        [6.6667, 2.6667]])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "o = torch.ones(T,T)\n",
        "a = torch.tril(o)\n",
        "w = a/ a.sum(1, keepdim=True)\n",
        "xbow2 = w @ x"
      ],
      "metadata": {
        "id": "bhQY2ictnAY0"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xbow2[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoBzhfS2nyhx",
        "outputId": "73ae69d5-da30-47ca-93e8-b3ae813049c3"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.3604, -1.5006],\n",
              "        [ 0.7338, -0.6960],\n",
              "        [ 0.5539, -0.1388],\n",
              "        [ 0.3351,  0.2609],\n",
              "        [ 0.1492,  0.3077],\n",
              "        [ 0.0892,  0.1931],\n",
              "        [-0.0351,  0.1505],\n",
              "        [ 0.0029,  0.3566]])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GagV8uobnzxQ",
        "outputId": "3e0052b4-3f7e-4b77-97a6-ce4291e54f12"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.3604, -1.5006],\n",
              "        [ 0.7338, -0.6960],\n",
              "        [ 0.5539, -0.1388],\n",
              "        [ 0.3351,  0.2609],\n",
              "        [ 0.1492,  0.3077],\n",
              "        [ 0.0892,  0.1931],\n",
              "        [-0.0351,  0.1505],\n",
              "        [ 0.0029,  0.3566]])"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFMBxlfan1Mp",
        "outputId": "17527d30-6644-4bb2-cdd2-d01c3f1a6800"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeevfYVvqsil",
        "outputId": "601d781b-2e12-4a1a-896a-911d6e32a88f"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiHfiI_zWjrA",
        "outputId": "102debf1-8899-49e5-d3e1-3477ea2f6304"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-Attention\n",
        "* 単一ヘッドを持つアテンションの実装\n",
        "* <u>すべての単語（文字）はその前までの単語（文字）の情報をデータによって変わる重みを考慮する</u>\n",
        "* Query：何を探しているか、 Key: それぞれの単語（文字）が何の情報を持っているか、Value：最終出力対象の材料（ベクトルの基底）\n",
        "→言い換えると、「Query（=問い合わせ）の各トークンに対応する、Value(=値)の重要度をKey（=鍵）を使って取り出し、Vの行ベクトルの線形結合として出力したもの」がSelf-Attentionである（参考：[Transformer: アテンションの計算式の意味を数理的に理解する](https://zenn.dev/bilzard/articles/49d453c7809be3)）\n",
        "\n",
        "$$\n",
        "Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n",
        "$$\n"
      ],
      "metadata": {
        "id": "veZoIch2pp8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(827)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqWakJa-V3T_",
        "outputId": "dd04c60d-1460-49ee-be3d-33278e01b8ac"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f70265232f0>"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "B, T, C = 4, 8, 32\n",
        "x = torch.randn(B, T, C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0HmP4qRV9Zg",
        "outputId": "c06ebbe5-3637-4de5-b512-6c1696e0c5fb"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "head_size = 16\n",
        "query = nn.Linear(C, head_size, bias=False) # input: 32, output 16 -> 4, 8, 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)"
      ],
      "metadata": {
        "id": "HQC5HW5oWaHN"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = query(x)\n",
        "k = key(x)\n",
        "v = value(x)"
      ],
      "metadata": {
        "id": "DolGumkHWa3-"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6feU0_6SW1IT",
        "outputId": "7965494e-cd7f-45b4-c295-1fa9b9d63e66"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iT-nd36NW2pv",
        "outputId": "516fe916-1af6-48f8-ac7a-d0e5c7ee7c0d"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k.transpose(-2, -1).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhpiH76ZW_Ny",
        "outputId": "8e35da18-e372-4af2-8d07-aa966c2291b6"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 16, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(q@k.transpose(-2, -1)).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CEOCiloWu73",
        "outputId": "c63f7631-adf4-42b0-8296-e8a290b0836c"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = head_size\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "m27BgL3KyoGu"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_k ** -0.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdSSih49yzBj",
        "outputId": "83b766c0-d778-4890-bffb-931804d759c4"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.25"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1/np.sqrt(d_k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqIrUGjWy0m4",
        "outputId": "e18cdb0d-d404-4d58-ac16-b787d1f3381e"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.25"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei = q@k.transpose(-2, -1) * (1/np.sqrt(d_k))\n",
        "# wei = torch.zeros((T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=1)\n",
        "xbow4 = wei @ v"
      ],
      "metadata": {
        "id": "I3nEiTe3Vzmp"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xbow4[0]"
      ],
      "metadata": {
        "id": "dNDVZjb6XyVt",
        "outputId": "9a73a903-fa1a-4266-afd2-7dd4ec7055dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-6.4943e-02, -8.6644e-02,  1.4013e-01,  1.1231e-02, -3.5064e-02,\n",
              "          1.4771e-03,  6.7439e-03, -7.1404e-02,  5.5764e-02,  8.6212e-02,\n",
              "          4.1400e-02,  8.4238e-02, -3.8881e-02,  7.9299e-03, -1.1986e-01,\n",
              "          3.0718e-02],\n",
              "        [ 2.0652e-02,  7.9037e-03,  9.5691e-02, -2.9729e-02,  4.0313e-02,\n",
              "          7.2093e-02, -9.7897e-02, -3.5155e-02,  1.6949e-01,  9.6479e-02,\n",
              "          9.5388e-02,  1.3727e-01, -1.1982e-02, -4.1898e-02, -1.9433e-01,\n",
              "          3.3020e-02],\n",
              "        [ 1.4957e-01,  1.0520e-01,  2.0257e-01, -1.0888e-01,  1.0346e-01,\n",
              "          2.1323e-01, -2.3184e-01,  5.2117e-02,  4.2328e-01,  1.8898e-01,\n",
              "          1.3357e-01,  2.0639e-01,  1.9878e-01,  2.6737e-01, -1.9527e-01,\n",
              "         -4.3224e-02],\n",
              "        [ 1.0034e-01,  3.2107e-01,  2.9599e-01, -3.2450e-02,  2.3063e-01,\n",
              "          3.1555e-01, -3.4271e-01,  1.1687e-01,  4.2867e-01,  7.0995e-03,\n",
              "          1.9509e-01, -1.0266e-01,  2.5036e-01,  3.5152e-01, -2.9578e-01,\n",
              "          2.8037e-02],\n",
              "        [-1.1911e-02,  2.5038e-01,  1.2999e-01, -4.6047e-02, -1.3316e-01,\n",
              "          2.7441e-01, -2.4573e-01,  1.3725e-01,  4.0597e-01, -3.1533e-02,\n",
              "         -9.4038e-03, -2.0412e-01,  1.0990e-01,  3.2097e-01, -2.0882e-01,\n",
              "          2.2349e-01],\n",
              "        [ 1.5558e-01,  2.7852e-02, -2.4227e-02, -2.5079e-01,  1.0345e-01,\n",
              "          2.4298e-01,  1.4749e-01,  9.6540e-02,  1.7027e-01, -2.5905e-01,\n",
              "         -1.4713e-01, -3.2873e-01,  4.1198e-01,  3.1917e-01, -9.6668e-02,\n",
              "          1.7193e-01],\n",
              "        [ 4.1991e-01,  2.1810e-01,  4.3869e-02,  4.5668e-01, -8.4376e-01,\n",
              "          3.2453e-01, -4.6012e-02,  7.2348e-01,  4.3585e-02, -6.1273e-01,\n",
              "         -4.3837e-01, -1.7153e-02,  3.7256e-01,  5.8715e-01,  3.3088e-01,\n",
              "          5.2403e-01],\n",
              "        [ 1.2664e-01, -5.2435e-01,  4.3454e-01,  5.4739e-01,  7.8875e-02,\n",
              "          7.0507e-01,  6.8927e-01,  4.8935e-01, -6.2568e-02, -1.4390e+00,\n",
              "         -1.4165e+00,  5.4886e-01,  1.7720e-02,  3.9261e-01,  7.3435e-01,\n",
              "          1.5179e+00]], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1,-0.2,-0.3,-0.2,0.5]), dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tL_cROJrofJM",
        "outputId": "2cd45f89-bf77-4d99-89b2-96d2d2ebd116"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2153, 0.1595, 0.1443, 0.1595, 0.3213])"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1,-0.2,-0.3,-0.2,0.5])*8, dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_CGcIltqcAA",
        "outputId": "87765a6f-b867-4c16-e049-d778f1d11ad8"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0388, 0.0035, 0.0016, 0.0035, 0.9525])"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q = torch.randn(B, T, d_k)\n",
        "k =torch.randn(B, T, d_k)"
      ],
      "metadata": {
        "id": "M0OZ5iuqzLQI"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = q @ k.transpose(-2,-1)*(1/np.sqrt(d_k))"
      ],
      "metadata": {
        "id": "rQ6HYO70zQCG"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0yg7ebpzTM3",
        "outputId": "6dac6c06-721b-48f5-fecc-dee2526c00ac"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9835)"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHAoSD4PznGg",
        "outputId": "974105eb-a15a-484b-f01f-c096ba1622d9"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0268)"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTr352DyzokA",
        "outputId": "b1dafee1-16ff-44ac-b0d1-7da5bd199c8c"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9743)"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 全コード(改善後）\n",
        "\n",
        "* positional encoding(まるごと体感したい、せとおおはし。 v.s. はしを使うのは難しい）\n",
        "* MultiHead attention（Scaled dot product attentionを複数結合）\n",
        "* dropoutを追加\n",
        "\n"
      ],
      "metadata": {
        "id": "lT6BOwNk35ff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# データ取得"
      ],
      "metadata": {
        "id": "tQgLYKwi_4LF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install janome\n",
        "!pip install jaconv\n",
        "\n",
        "import requests, bs4\n",
        "import re\n",
        "from janome.tokenizer import Tokenizer\n",
        "import jaconv\n",
        "import torch\n",
        "\n",
        "# よみがな（ひらがな）取得関数\n",
        "def get_yomigana(original_txt):\n",
        "  t = Tokenizer()\n",
        "  l = []\n",
        "  for w in t.tokenize(original_txt):\n",
        "    reading = w.reading\n",
        "    if  reading == '*':\n",
        "      l.append(w.base_form)\n",
        "    else:\n",
        "      l.append(reading)\n",
        "  hira = jaconv.kata2hira(''.join(l))\n",
        "  return hira\n",
        "\n",
        "# データ取得\n",
        "response = requests.get(\"https://www.aozora.gr.jp/cards/000148/files/789_14547.html\")\n",
        "soup = bs4.BeautifulSoup(response.content, \"html.parser\")\n",
        "elems = soup.select(\".main_text\")\n",
        "texts = ''.join([elm.text for elm in elems])\n",
        "pattern = re.compile(r\"\\n一\\n|\\r\\n|\\n|\\u3000\", re.MULTILINE | re.DOTALL)\n",
        "texts = re.sub(pattern, \"\", texts)\n",
        "texts = get_yomigana(texts)\n",
        "\n",
        "# すべてのユニークな文字（夏目漱石の「吾輩は猫であるに出現）\n",
        "chars = sorted(set(texts))\n",
        "ctoi = {c:i for (i, c) in enumerate(chars)}\n",
        "itoc = {i:c for (i, c) in enumerate(chars)}\n",
        "\n",
        "# エンコーダ・デコーダの定義\n",
        "encode = lambda s:[ctoi[c] for c in s]\n",
        "decode = lambda l:''.join([itoc[i] for i in l])\n",
        "\n",
        "# 訓練データ・検証用データの分け\n",
        "data = torch.tensor(encode(texts), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "vocab_size = len(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIA6nEd_8zLr",
        "outputId": "c010b0b4-ccc5-410f-c144-3b4ad38a784b"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: janome in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: jaconv in /usr/local/lib/python3.10/dist-packages (0.3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yniY7cKF09W",
        "outputId": "af0f69db-d89f-4d4b-bec8-c611671164be"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "673"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# モデル定義　→　訓練　→　1件推論"
      ],
      "metadata": {
        "id": "61xQ7JCr_6hY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# 結果再現用\n",
        "torch.manual_seed(827)\n",
        "\n",
        "# ハイパーパラメタ\n",
        "batch_size = 128 # いくつの独立したシーケンスを並列で扱うか\n",
        "block_size = 128 # コンテキストサイズ（チャンクサイズwhat is the maximum context length for predictions?\n",
        "max_iters = 1000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------\n",
        "\n",
        "# ミニバッチ取得\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 入力サイズ (batch_size, time-step, channels)\n",
        "        # 出力しず (batch_size, time-step, head_size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,head_size)\n",
        "        q = self.query(x) # (B,T,head_size)\n",
        "        # アテンションスコアの計算(\"affinities:親和性\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # 値の重みつき集約（値からアテンションスコアに基づき対応する成分を取り出す）の実行\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" 並行して計算可能なセルフアテンションヘッド \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" 非線形性をもつようにした全結合層 \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 各トークンは、ルックアップテーブルから次のトークンのロジットを直接読み出す\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx：(B, T)\n",
        "        for _ in range(max_new_tokens):\n",
        "            # idxを最後のブロック・サイズ・トークンにクロップする。\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # 推論\n",
        "            logits, loss = self(idx_cond)\n",
        "            # 最後のタイムステップのみに着目（言語モデルなので）\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # ソフトマックスを適用して確率を取得\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # 多項分布からサンプリング\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # シーケンスにサンプリングしたインデックスを追加\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# モデルのパラメタ数を標準出力\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# 最適化関数は、AdamWを利用\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # eval_intervalの間隔ごとにlossの平均を評価する。\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, validation loss {losses['val']:.4f}\")\n",
        "\n",
        "    # ミニバッチを取得\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # 誤差逆伝播\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# サンプルを生成してみる。\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c4Z8bVdzqmw",
        "outputId": "e6d94ee3-b5e4-429d-c1d6-c4ce494bee65"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11.207329 M parameters\n",
            "step 0: train loss 6.4870, validation loss 6.4869\n",
            "step 500: train loss 2.6419, validation loss 2.8797\n",
            "step 999: train loss 2.0711, validation loss 2.7962\n",
            " はっけんしつするくらいだ。これをしらずにむかっておこらんのである。しゅうめいはちきんじょうのやくぶふかないしゅつのかひかってくれればだくのはかっしである。ぬぼといおおいのずからかぜをひくだすにかにているにふうふうちがこんだけしかわきのけっかところでない。ただどころのためにねをひらいたしのはなしをして、はなしをすることはまたようにおもって、うまれごとくべしようかすが、いっしのくそをあげきしまわりのなかへはたかくけた。かおのしむかいそうとひぐるものところはろんぶんよりもないとへいぜんぼえてきだしたてぁい！としょうじがつまらあけて「しはそれじゃきみのなかむろはくくびぐるが、ぼりのまんぼうさんみずをねむくのでは、みやむをえずいぶんだけのらいたりで、すこぶるはたね。ひまのそれからはんへんぞんぶつのくらいなこころぼそいおんがすました。ぷれーとすおさめしをてそんこ」「かまわんとするんさ。そばめしをぼくらそんすってるんだな」くさいくんはへびめしめしくっている。迷亭はまったそうだとおもいおおやこのこえがする。迷にっせぬとおりなげんのほうへまいぎわに。「ともえかにていしゃさけごのないへいにすまんまよんです\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# トークナイザーを単語ベースに切り替えて訓練・推論してみる。"
      ],
      "metadata": {
        "id": "cUNJpmwFsN5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install janome\n",
        "from janome.tokenizer import Tokenizer\n",
        "\n",
        "# 単語の表層系取得関数\n",
        "def get_surface(original_txt):\n",
        "  t = Tokenizer()\n",
        "  l = []\n",
        "  for w in t.tokenize(original_txt):\n",
        "    reading = w.reading\n",
        "    l.append(w.surface)\n",
        "  return l\n",
        "\n",
        "words = get_surface(\"すももももももももものうち\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DQdsWA_vb7r",
        "outputId": "297d616f-4b7f-443c-ef08-e75a21a8b5fb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: janome in /usr/local/lib/python3.10/dist-packages (0.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(set(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbIh5y3RvrX0",
        "outputId": "83edc7ff-4f4f-41aa-d53f-15f4cce95c23"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['うち', 'すもも', 'の', 'も', 'もも']"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import requests, bs4\n",
        "import re\n",
        "\n",
        "import jaconv\n",
        "import torch\n",
        "\n",
        "# データ取得\n",
        "response = requests.get(\"https://www.aozora.gr.jp/cards/000148/files/789_14547.html\")\n",
        "soup = bs4.BeautifulSoup(response.content, \"html.parser\")\n",
        "elems = soup.select(\".main_text\")\n",
        "texts = ''.join([elm.text for elm in elems])\n",
        "pattern = re.compile(r\"\\n一\\n|\\r\\n|\\n|\\u3000\", re.MULTILINE | re.DOTALL)\n",
        "texts = re.sub(pattern, \"\", texts)\n",
        "texts = get_surface(texts)\n",
        "\n",
        "# すべてのユニークな単語（表層系）（夏目漱石の「吾輩は猫であるに出現）\n",
        "words = sorted(set(texts))\n",
        "wtoi = {w:i for (i, w) in enumerate(words)}\n",
        "itow = {i:w for (i, w) in enumerate(words)}\n",
        "\n",
        "# エンコーダ・デコーダの定義\n",
        "encode = lambda s:[wtoi[w] for w in s]\n",
        "decode = lambda l:''.join([itow[i] for i in l])\n",
        "\n",
        "# 訓練データ・検証用データの分け\n",
        "data = torch.tensor(encode(texts), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "vocab_size = len(words)\n",
        "vocab_size"
      ],
      "metadata": {
        "id": "MV48pj9H4JP9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad027ba3-5cc4-49d2-e5d7-b4d740258802"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13622"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# 結果再現用\n",
        "torch.manual_seed(827)\n",
        "\n",
        "# ハイパーパラメタ\n",
        "batch_size = 128 # いくつの独立したシーケンスを並列で扱うか\n",
        "block_size = 128 # コンテキストサイズ（チャンクサイズwhat is the maximum context length for predictions?\n",
        "max_iters = 500\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------\n",
        "\n",
        "# ミニバッチ取得\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 入力サイズ (batch_size, time-step, channels)\n",
        "        # 出力しず (batch_size, time-step, head_size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,head_size)\n",
        "        q = self.query(x) # (B,T,head_size)\n",
        "        # アテンションスコアの計算(\"affinities:親和性\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # 値の重みつき集約（値からアテンションスコアに基づき対応する成分を取り出す）の実行\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" 並行して計算可能なセルフアテンションヘッド \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" 非線形性をもつようにした全結合層 \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 各トークンは、ルックアップテーブルから次のトークンのロジットを直接読み出す\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx：(B, T)\n",
        "        for _ in range(max_new_tokens):\n",
        "            # idxを最後のブロック・サイズ・トークンにクロップする。\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # 推論\n",
        "            logits, loss = self(idx_cond)\n",
        "            # 最後のタイムステップのみに着目（言語モデルなので）\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # ソフトマックスを適用して確率を取得\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # 多項分布からサンプリング\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # シーケンスにサンプリングしたインデックスを追加\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# モデルのパラメタ数を標準出力\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# 最適化関数は、AdamWを利用\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # eval_intervalの間隔ごとにlossの平均を評価する。\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, validation loss {losses['val']:.4f}\")\n",
        "\n",
        "    # ミニバッチを取得\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # 誤差逆伝播\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# サンプルを生成してみる。\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCfA86aet7jg",
        "outputId": "e53abdd6-07e7-4de2-b931-e36c910df79f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21.16511 M parameters\n",
            "step 0: train loss 9.5784, validation loss 9.5785\n",
            "step 499: train loss 2.6648, validation loss 5.6782\n",
            " は静粛匹もちらちらと思っていたが、この点において人曲直を隆盛の見込の折れる者と臥竜訛りか向う咎入から横注するの傍であるまいと思うとその例の光線を引っ大会を只卸せべき五体者だがする戸棚を単簡である。こいつの妻君を見たに動いている。この鼻だの、奇麗陋劣な学の研究併発しなくしないところで、主人は冷評に対して悠然と御台ストーブの上の警察の境のヴァイオリンはなかったので、ある不平家の蓋を屋へ出る者がない。この間出掛けようと毛の部屋から燃える事を二絃琴の今度はトントンと吾輩の穴へ虎湯気が出さええ侍登った時の飛び出しそうした。それから笑いうたって天下の旧友とする。なぜ桶に応じして分けるが、そっとあっと大きな了見か、この音をこっちを潜ったしている。この時他の愚考注するのはうと知れんから」と同時に黒い懸命にない。それがない。一匹鼠を表するためにも三人の人も何動物にして心も来る以上は猫でもあるから、猫浮かれらるるにはならん。吾輩にも何ともないと云う事ならやはり猫のもなく金魚麩の左の夫婦と号するのだから、ついた時は少々極りがしたたか御代である。それだしないのように、実はそのは有名な顔の遊戯さっぱり出来れば、例のごとく発明に、両親が善い晴な近所である。ただこの婆な閑中自へ行く訳はない。浅黄時分の二結構よと細君は奮然バスを添うて泊りがけにして来た。すん子が出て、や吾坊主の運動中に見易きどうせガル孔明の意を取られて死物狂いの主人の礼も太平の所作である。知らん顔をしてやろう。これを持って焚けしかしながら少し有名な理窟は分らない上である事実は飛んだ事だ。しかしその法螺が出来ない泥棒ますか、嬉しいと思うとはきっとおこせと云った。飛び込みながらそのらして来た方から、遅くなるまでは廃してそのように手数が付いたが好きだから、この希望を処理して、少なくともこれがしつこくてやめなければならん。吾輩\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_aYU9vlwwypo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}